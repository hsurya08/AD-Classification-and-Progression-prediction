{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary lib\nimport numpy as np\nfrom pandas import read_csv\nimport pandas as pd\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport pickle\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.134904Z","iopub.execute_input":"2023-04-19T03:39:24.135348Z","iopub.status.idle":"2023-04-19T03:39:24.143018Z","shell.execute_reply.started":"2023-04-19T03:39:24.135299Z","shell.execute_reply":"2023-04-19T03:39:24.141607Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Helping functions","metadata":{}},{"cell_type":"code","source":"# Function to determine unique values in a dataframe's column based on the column's name.\ndef unique_value_function(df, feature_name):\n    if feature_name in df.columns:\n        _values = df[[feature_name]]\n        unique_values = _values.values\n        unique_values = np.unique(unique_values)\n        num_of_unique_values = np.unique(unique_values).shape[0]\n        #print('Number of unique values in '+feature_name+' is: ', num_of_unique_values)\n        return unique_values\n    else:\n        print(str(feature_name)+' is not a feature name in this dataframe.')\n        return -1","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.145249Z","iopub.execute_input":"2023-04-19T03:39:24.145685Z","iopub.status.idle":"2023-04-19T03:39:24.160013Z","shell.execute_reply.started":"2023-04-19T03:39:24.145648Z","shell.execute_reply":"2023-04-19T03:39:24.158615Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Function to check if the dataset has all necessary features and been preprocessed carefully.\ndef check_longitudinal_dataset(df):\n    # chech if the dataframe is empty\n    if df.empty:\n        print('Dataset is empty.')\n        return -1\n    \n    # Check for missing values\n    if df.isnull().sum().any():\n        print('Dataset has NAN values and needs to preprocess.')\n        return -1\n    \n    # Check for the existence of important features\n    features_name = df.columns\n    if not('RID' in features_name) or not('VISCODE' in features_name) or not('DX' in features_name):\n        print('Dataset does not have necessary feature/s')\n        return -1\n    \n    # chech if the RID's is numeric\n    if not(is_numeric_dtype(df['RID'])):\n        print('Patient ID should be numeric.')\n        return -1\n    \n    # Check for the existence of only Dementia and MCI as diagnosis\n    unique_diagnosis = unique_value_function(df, 'DX')\n    if (len(unique_diagnosis) != 2) or not('Dementia' in unique_diagnosis) or not('MCI' in unique_diagnosis):\n        print('Dataset does not have correct diagnosis or unique number of diagnosis.')\n        return -1\n    \n    # Check if the dataframe has at least one longitudinal feature other than RID, VISCODE, and DX\n    if not(df.shape[1] > 3):\n        print('Dataset does not have enough features')\n        return -1\n    \n    # Check if the longitudinal data is numiric\n    features_name = df.columns\n    flag = False\n    for i in range(len(features_name)):\n        if not(features_name[i] in ['RID', 'DX', 'VISCODE']):\n            if not(is_numeric_dtype(df[features_name[i]])):\n                flag = True\n    if flag:\n        print('Data should be numeric.')\n        return -1\n            \n    \n    # return 1 if the dataset is ready\n    return 1","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.161937Z","iopub.execute_input":"2023-04-19T03:39:24.162284Z","iopub.status.idle":"2023-04-19T03:39:24.176195Z","shell.execute_reply.started":"2023-04-19T03:39:24.162252Z","shell.execute_reply":"2023-04-19T03:39:24.174871Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Function to check if the dataset has all necessary features and been preprocessed carefully.\ndef check_demographic_dataset(df):\n    # chech if the dataframe is empty\n    if df.empty:\n        print('Dataset is empty.')\n        return -1\n    \n    # Check for missing values\n    if df.isnull().sum().any():\n        print('Dataset has NAN values and needs to preprocess.')\n        return -1\n    \n    # Check for the existence of important features\n    features_name = df.columns\n    if not('RID' in features_name):\n        print('Dataset does not have necessary feature/s')\n        return -1\n    \n    # chech if the RID's is numeric\n    if not(is_numeric_dtype(df['RID'])):\n        print('Patient ID should be numeric.')\n        return -1\n    \n    # Check if the dataframe has at least one demographic feature other than RID\n    if not(df.shape[1] > 1):\n        print('Dataset does not have enough features')\n        return -1\n    \n    # return 1 if the dataset is ready\n    return 1","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.178433Z","iopub.execute_input":"2023-04-19T03:39:24.178819Z","iopub.status.idle":"2023-04-19T03:39:24.197439Z","shell.execute_reply.started":"2023-04-19T03:39:24.178779Z","shell.execute_reply":"2023-04-19T03:39:24.196073Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Function to prepare longitudinal data (VISCODE): Modify the VISCODE to have only longitudnal features\ndef visit_code_preperation(df):\n    # replace 'bl' with 0\n    df['VISCODE'] = df['VISCODE'].replace('bl', 0)\n\n    # extract the integer part of 'm' values\n    m_values = df['VISCODE'].str.extract(r'm(\\d+)')\n\n    # replace 'm' values with integer part\n    df['VISCODE'] = pd.to_numeric(m_values.squeeze(), errors='coerce').fillna(df['VISCODE']).astype(int)\n    \n    df['TimeDiff'] = df['VISCODE']\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.199205Z","iopub.execute_input":"2023-04-19T03:39:24.199570Z","iopub.status.idle":"2023-04-19T03:39:24.216123Z","shell.execute_reply.started":"2023-04-19T03:39:24.199535Z","shell.execute_reply":"2023-04-19T03:39:24.214841Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def num_patients_visits_function(df, _list):\n    # Calculate maximum number of visits\n    visit_size = 0\n    for i in range(len(_list)):\n        temp_data = df[df[\"RID\"] == _list[i]]\n        size = (temp_data.shape)[0]\n        if size > visit_size:\n            visit_size = size\n\n    # Calculate how many patients in each number of visit groups for removed patients\n    visit_size = np.zeros((visit_size),int)\n    for i in range(len(_list)):\n        temp_data = df[df[\"RID\"] == _list[i]]\n        size = (temp_data.shape)[0]\n        visit_size[size-1] = visit_size[size-1] + 1\n\n    for i in range(len(visit_size)):\n        print(i+1,'_Visit = ', visit_size[i])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.217881Z","iopub.execute_input":"2023-04-19T03:39:24.218326Z","iopub.status.idle":"2023-04-19T03:39:24.231679Z","shell.execute_reply.started":"2023-04-19T03:39:24.218276Z","shell.execute_reply":"2023-04-19T03:39:24.230580Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Encoding Diagnosis DX (MCI = 0 and Dementia = 1)\ndef encode_diagnosis(df):\n    for i in range(len(df)):\n        if df.loc[i, 'DX'] == 'MCI':\n            df.loc[i, 'DX'] = 0\n        else:\n            df.loc[i, 'DX'] = 1\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.233344Z","iopub.execute_input":"2023-04-19T03:39:24.233802Z","iopub.status.idle":"2023-04-19T03:39:24.252274Z","shell.execute_reply.started":"2023-04-19T03:39:24.233734Z","shell.execute_reply":"2023-04-19T03:39:24.250779Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Normalize only the MRI features with respect to ICV\ndef normalize_features(df):\n    mri_columns = df.columns[7:13]\n    df[mri_columns] = df[mri_columns].div(df['ICV'], axis = 0)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.253457Z","iopub.execute_input":"2023-04-19T03:39:24.253945Z","iopub.status.idle":"2023-04-19T03:39:24.265745Z","shell.execute_reply.started":"2023-04-19T03:39:24.253896Z","shell.execute_reply":"2023-04-19T03:39:24.264163Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Normalize longitudinal data using min-max normalization\ndef min_max_normalization(df):\n    columns = list(df.columns)\n    new_arrangment_for_columns = ['RID','VISCODE']\n    for col in columns:\n        if not(col in new_arrangment_for_columns) and col != 'DX':\n            new_arrangment_for_columns.append(col)\n    new_arrangment_for_columns.append('DX')\n    \n    df = df[new_arrangment_for_columns]\n    \n    for i in range(2, len(new_arrangment_for_columns)-1):   \n        temp_data = df.iloc[:,i]\n        max_value = temp_data.max()\n        min_value = temp_data.min()\n        for j in range(len(df)):\n            df.iat[j, i] = (df.iloc[j, i]-min_value)/(max_value - min_value)\n            \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.268904Z","iopub.execute_input":"2023-04-19T03:39:24.269364Z","iopub.status.idle":"2023-04-19T03:39:24.283798Z","shell.execute_reply.started":"2023-04-19T03:39:24.269325Z","shell.execute_reply":"2023-04-19T03:39:24.282394Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def remove_icv(df):\n    df = df.drop('ICV', axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.285474Z","iopub.execute_input":"2023-04-19T03:39:24.285998Z","iopub.status.idle":"2023-04-19T03:39:24.297123Z","shell.execute_reply.started":"2023-04-19T03:39:24.285941Z","shell.execute_reply":"2023-04-19T03:39:24.296133Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Function to group patients together based on number of visits they have\ndef group_patients_according_number_of_visits(df):\n    unique_id = unique_value_function(df, 'RID')\n    visits_dic = {}\n    \n    for i in range(len(unique_id)):\n        temp_data = df[df[\"RID\"] == unique_id[i]]\n        temp_data.reset_index(drop=True,inplace=True)\n        size = temp_data.shape[0]\n        \n        if size in visits_dic:\n            visits_dic[size] = pd.concat([visits_dic[size], temp_data])\n            visits_dic[size].reset_index(drop=True, inplace=True)\n        else:\n            visits_dic[size] = temp_data\n            \n    # sort the dictionary based on the key\n    sorted_dic = {}\n    for key in sorted(visits_dic):\n        sorted_dic[key] = visits_dic[key]\n    return sorted_dic","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.298740Z","iopub.execute_input":"2023-04-19T03:39:24.299194Z","iopub.status.idle":"2023-04-19T03:39:24.311031Z","shell.execute_reply.started":"2023-04-19T03:39:24.299158Z","shell.execute_reply":"2023-04-19T03:39:24.310020Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Function to transpose the longitudinal dataset\ndef transpose_longitudinal_data(group_longitudinal_data_dic, features_to_be_in_columns):\n    transposed_lonitudinal_data_dic = {}\n    for key in (group_longitudinal_data_dic):\n        transposed_lonitudinal_data_dic[key] = group_longitudinal_data_dic[key].pivot(index = 'RID', columns= 'VISCODE',\n                                                                                      values= features_to_be_in_columns)\n        \n    new_columns_names_dic = {}\n    for key in (group_longitudinal_data_dic):\n        new_columns_names_dic[key] = ['RID']\n    for key in (group_longitudinal_data_dic):\n        time_points = key\n        \n        for i in range(time_points):\n            for j in range(1, int(len(transposed_lonitudinal_data_dic[key].columns)/time_points+1)):\n                column_idex = i + (key * j) - key\n                new_columns_names_dic[key].append(transposed_lonitudinal_data_dic[key].columns[column_idex][0] + '_'+ str(transposed_lonitudinal_data_dic[key].columns[column_idex][1]))\n                \n    final_longitudinal_data_dic = {}\n    for key in (group_longitudinal_data_dic):\n        time_points = key\n        unique_rid = unique_value_function(group_longitudinal_data_dic[key], 'RID')\n\n        final_longitudinal_data_dic[key] = pd.DataFrame(columns = new_columns_names_dic[key])\n        for x in range(len(transposed_lonitudinal_data_dic[key])):\n            new_time_point_data = []\n            new_time_point_data.append(unique_rid[x])\n            for i in range(time_points):\n                for j in range(1, int(len(transposed_lonitudinal_data_dic[key].columns)/time_points+1)):\n                    column_idex = i + (time_points * j) - time_points\n                    new_time_point_data.append(transposed_lonitudinal_data_dic[key].iloc[x, column_idex])\n            final_longitudinal_data_dic[key].loc[len(final_longitudinal_data_dic[key])] = new_time_point_data\n            \n    return final_longitudinal_data_dic","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.312357Z","iopub.execute_input":"2023-04-19T03:39:24.312808Z","iopub.status.idle":"2023-04-19T03:39:24.332088Z","shell.execute_reply.started":"2023-04-19T03:39:24.312722Z","shell.execute_reply":"2023-04-19T03:39:24.331042Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Apply one-hot-encoding only for categorical demographics features\ndef demographic_one_hot_encoding(demographic_df):\n    \n    demographic_data = demographic_df\n    categorical_columns = []\n    all_columns = list(demographic_data.columns)\n    for i in range(len(all_columns)):\n        if all_columns[i] != 'RID' and all_columns[i] != 'PTEDUCAT':\n            categorical_columns.append(all_columns[i])\n\n    for c in range(len(categorical_columns)):\n        tempdf = pd.get_dummies(demographic_data[categorical_columns[c]], prefix=categorical_columns[c])\n        demographic_data = pd.concat([demographic_data, tempdf], axis=1)\n        demographic_data = demographic_data.drop(columns=categorical_columns[c])\n\n    categorical_columns_will_be_used = list(demographic_data.columns)\n\n    temp_columns = demographic_data.columns\n    temp_keep_these_columns = []\n    for c in range(len(temp_columns)):\n        for k in range(len(categorical_columns_will_be_used)):\n            if categorical_columns_will_be_used[k] in temp_columns[c]:\n                temp_keep_these_columns.append(temp_columns[c])\n    demographic_data = demographic_data[temp_keep_these_columns]\n    \n    return demographic_data","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.333657Z","iopub.execute_input":"2023-04-19T03:39:24.334174Z","iopub.status.idle":"2023-04-19T03:39:24.352480Z","shell.execute_reply.started":"2023-04-19T03:39:24.334128Z","shell.execute_reply":"2023-04-19T03:39:24.351311Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Function to split the longitudinal data into training and test data 70% and 30% respectively\ndef split_longitudinal_data(longitudinal_data_dic):\n    train_data = {}\n    test_data = {}\n    \n    for key in longitudinal_data_dic:\n        if key == 15:\n            continue\n        X_train, X_test = train_test_split(longitudinal_data_dic[key], test_size=0.3, random_state=42)\n        if key in train_data:\n            train_data[key] = pd.concat([train_data[key], X_train])\n            train_data[key].reset_index(drop=True, inplace=True)\n            \n            test_data[key] = pd.concat([test_data[key], X_test])\n            test_data[key].reset_index(drop=True, inplace=True)\n        else:\n            train_data[key] = X_train\n            test_data[key] = X_test\n            \n    return train_data, test_data","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.353522Z","iopub.execute_input":"2023-04-19T03:39:24.353863Z","iopub.status.idle":"2023-04-19T03:39:24.371755Z","shell.execute_reply.started":"2023-04-19T03:39:24.353831Z","shell.execute_reply":"2023-04-19T03:39:24.370322Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Global veriable\nnum_features_in_each_time_step = 1\ntime_steps = 2\ndemographic_features = 1\n\n# Training data lists\ndataset = []\ndemographic_train = []\ntarget_1 = []\n\n# Test data lists\nTestset = []\ndemographic_test = []\ntarget_2 = []","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.373458Z","iopub.execute_input":"2023-04-19T03:39:24.373857Z","iopub.status.idle":"2023-04-19T03:39:24.386536Z","shell.execute_reply.started":"2023-04-19T03:39:24.373819Z","shell.execute_reply":"2023-04-19T03:39:24.385345Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Function to create training lists (longitudinal, demographics, label)\ndef create_train_lists(longitudinal_df, demographic_df, tp, ftp):\n    global dataset\n    global demographic_train\n    global target_1\n    \n    uid = unique_value_function(longitudinal_df, 'RID')\n    temp_demographic_df = pd.DataFrame(columns = list(demographic_df.columns)[1::])\n    for i in range(len(uid)):\n        temp_data = demographic_df[demographic_df[\"RID\"] == uid[i]]\n        temp_data.reset_index(drop=True, inplace=True)\n        new_row = temp_data.iloc[0,1:]\n        temp_demographic_df.loc[len(temp_demographic_df)] = new_row\n    \n    \n    num_feature_in_tp = num_features_in_each_time_step\n    df1 = longitudinal_df[longitudinal_df.columns]\n    \n    diagnosis_columns_names = []\n    all_columns = list(longitudinal_df.columns)\n    for i in range(len(all_columns)):\n        if 'DX_' in all_columns[i]:\n            diagnosis_columns_names.append(all_columns[i])\n    \n    # dataframe at least has one time point for data and ftp for prediction \n    if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= ftp+1:\n        Features = df1.loc[:, ~df1.columns.isin(diagnosis_columns_names)]\n        \n        Labels = df1.loc[:, df1.columns.isin(diagnosis_columns_names)]\n\n                    \n        # dataframe has tp and ftp\n        if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= tp+ftp:\n            for i in range(len(df1)):\n                dataset.append(list(Features.iloc[i,1:tp*num_feature_in_tp+1]))\n                demographic_train.append(list(temp_demographic_df.iloc[i,:]))\n                target_1.append(list(Labels.iloc[i,tp:tp+ftp]))\n        else:\n            for i in range(len(df1)):\n                dataset.append(list(Features.iloc[i,1:Features.shape[1] - (ftp*num_feature_in_tp)]))\n                demographic_train.append(list(temp_demographic_df.iloc[i,:]))\n                target_1.append(list(Labels.iloc[i,Labels.shape[1]-ftp:]))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.388122Z","iopub.execute_input":"2023-04-19T03:39:24.388494Z","iopub.status.idle":"2023-04-19T03:39:24.410717Z","shell.execute_reply.started":"2023-04-19T03:39:24.388457Z","shell.execute_reply":"2023-04-19T03:39:24.409314Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Function to create test lists (longitudinal, demographics, label)\ndef create_test_lists(longitudinal_df, demographic_df, tp, ftp):\n    global Testset\n    global target_2\n    global demographic_test\n    global target_2_prev\n    \n    uid = unique_value_function(longitudinal_df, 'RID')\n    temp_demographic_df = pd.DataFrame(columns = list(demographic_df.columns)[1::])\n    for i in range(len(uid)):\n        temp_data = demographic_df[demographic_df[\"RID\"] == uid[i]]\n        temp_data.reset_index(drop=True, inplace=True)\n        new_row = temp_data.iloc[0,1:]\n        temp_demographic_df.loc[len(temp_demographic_df)] = new_row\n    \n    \n    num_feature_in_tp = num_features_in_each_time_step\n    df1 = longitudinal_df[longitudinal_df.columns]\n    \n    diagnosis_columns_names = []\n    all_columns = list(longitudinal_df.columns)\n    for i in range(len(all_columns)):\n        if 'DX_' in all_columns[i]:\n            diagnosis_columns_names.append(all_columns[i])\n    \n    # dataframe must have tp+ftp visits \n    if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= tp+ftp:\n        Features = df1.loc[:, ~df1.columns.isin(diagnosis_columns_names)]\n        \n        Labels = df1.loc[:, df1.columns.isin(diagnosis_columns_names)]\n\n        for i in range(len(df1)):\n            Testset.append(list(Features.iloc[i,1:tp*num_feature_in_tp+1]))\n            demographic_test.append(list(temp_demographic_df.iloc[i,:]))\n            target_2.append(list(Labels.iloc[i,tp:tp+ftp]))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.412332Z","iopub.execute_input":"2023-04-19T03:39:24.413118Z","iopub.status.idle":"2023-04-19T03:39:24.434245Z","shell.execute_reply.started":"2023-04-19T03:39:24.413061Z","shell.execute_reply":"2023-04-19T03:39:24.433130Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Function to create the train dataset\ndef create_dataset_train(train_data_list, ts, fts, demographic_df):\n    global time_steps\n    global demographic_features\n    global dataset\n    global demographic_train\n    global target_1\n    \n    dataset = []\n    demographic_train = []\n    target_1 = []\n\n    time_steps = ts\n    \n    train_df_list = []\n    \n    for i in range(len(train_data_list)):\n        train_df_list.append(train_data_list[i])\n    \n    #create_train_lists(df1_train,time_steps)\n    for i in range(len(train_df_list)):\n        create_train_lists(train_df_list[i], demographic_df, time_steps, fts)\n        \n    # Train Padding\n    padded1 = pad_sequences(dataset, padding='post',dtype='float', value=-1)\n\n    num_samples = len(padded1)\n    num_features = padded1.shape[1]\n    time_steps = int(num_features / num_features_in_each_time_step)\n    dataset = padded1\n    padded_ = pad_sequences(target_1, padding='post',dtype='float', value=-1)\n    target_1 = padded_\n    num_labels = padded_.shape[1]\n    # data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n    dataset = np.reshape(dataset, (num_samples, time_steps, num_features_in_each_time_step))\n    target_1 = np.reshape(target_1, (num_samples, num_labels, 1))\n\n    \n    return dataset, target_1, demographic_train","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.436421Z","iopub.execute_input":"2023-04-19T03:39:24.436987Z","iopub.status.idle":"2023-04-19T03:39:24.455375Z","shell.execute_reply.started":"2023-04-19T03:39:24.436934Z","shell.execute_reply":"2023-04-19T03:39:24.454085Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Function to create test dataset\ndef create_dataset_test(test_data_list, ts, fts, demographic_df):\n    global time_steps\n    global demographic_features\n    global Testset\n    global target_2\n    global demographic_test\n    global target_2_prev\n    \n    Testset = []\n    target_2 = []\n    demographic_test = []\n    target_2_prev = []\n\n    time_steps = ts\n    \n    test_df_list = []\n    \n    for i in range(len(test_data_list)):\n        test_df_list.append(test_data_list[i])\n    \n    #create_train_lists(df1_train,time_steps)\n    for i in range(len(test_df_list)):\n        create_test_lists(test_df_list[i], demographic_df, time_steps, fts)\n        \n    # Test Padding\n    padded2 = pad_sequences(Testset, padding='post',dtype='float', value=-1)\n\n    T_num_samples = len(padded2)\n    Testset = padded2\n    target_2 = np.array(target_2)\n\n    # Test data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n    Testset = np.reshape(Testset, (T_num_samples, time_steps, num_features_in_each_time_step))\n    target_2 = np.reshape(target_2, (T_num_samples, fts, 1))\n\n    \n    return Testset, target_2, demographic_test","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.457059Z","iopub.execute_input":"2023-04-19T03:39:24.457497Z","iopub.status.idle":"2023-04-19T03:39:24.475327Z","shell.execute_reply.started":"2023-04-19T03:39:24.457460Z","shell.execute_reply":"2023-04-19T03:39:24.473978Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.479365Z","iopub.execute_input":"2023-04-19T03:39:24.480112Z","iopub.status.idle":"2023-04-19T03:39:24.496958Z","shell.execute_reply.started":"2023-04-19T03:39:24.480066Z","shell.execute_reply":"2023-04-19T03:39:24.495632Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"# Main function\ndef pkl_files_creator():\n    global num_features_in_each_time_step\n    global time_steps\n    global demographic_features\n    # Read csv files for longitudinal and demographic data\n    # Longitudinal data\n    file_name = '/kaggle/input/modified-dataset/17thapril_longitudinal.csv'\n    longitudinal_df = read_csv(file_name, header=0)\n\n    # Demographic data\n    file_name = '/kaggle/input/modified-dataset/17thapril_demographic.csv'\n    demographic_df = read_csv(file_name, header=0)\n    \n    # working on longitudinal data\n    if check_longitudinal_dataset(longitudinal_df) == -1:\n        return -1\n    if check_demographic_dataset(demographic_df) == -1:\n        return -1\n    longitudinal_df = visit_code_preperation(longitudinal_df)\n    longitudinal_df = encode_diagnosis(longitudinal_df)\n    longitudinal_df = normalize_features(longitudinal_df)\n    longitudinal_df = min_max_normalization(longitudinal_df)\n    longitudinal_df = remove_icv(longitudinal_df)\n    longitudinal_df_dic = group_patients_according_number_of_visits(longitudinal_df)\n    features_to_be_in_columns = 0\n    for key in longitudinal_df_dic:\n        features_to_be_in_columns = list(longitudinal_df_dic[key].columns)[2::]\n        break\n    longitudinal_df_dic = transpose_longitudinal_data(longitudinal_df_dic, features_to_be_in_columns)\n    longitudinal_train_data, longitudinal_test_data = split_longitudinal_data(longitudinal_df_dic)\n    \n    # working on demographic data\n    demographic_df = demographic_one_hot_encoding(demographic_df)\n    \n    # user choises\n    number_of_training_visits = input(\"Please enter number of visits that you want to use for training the model:\\n\")\n    while not(number_of_training_visits.isdigit()):\n        number_of_training_visits = input(\"Please enter integer values:\\n\")\n    number_of_training_visits = int(number_of_training_visits)\n    \n    number_of_future_visits = input(\"Please enter number of future visits that you want to predict:\\n\")\n    while not(number_of_future_visits.isdigit()):\n        number_of_future_visits = input(\"Please enter integer values:\\n\")\n    number_of_future_visits = int(number_of_future_visits)\n    \n    key_list = []\n    for key in longitudinal_df_dic:\n        key_list.append(key)\n    minimum_visit = key_list[0]\n    maximum_visit = key_list[-1]\n    if (number_of_future_visits + number_of_training_visits) > maximum_visit:\n        print('The Dataset does not have enough visits for this selection')\n        return -1\n    if number_of_training_visits < minimum_visit or number_of_future_visits < 1:\n        print('Wrong selection')\n        return -1\n    \n    num_features_in_each_time_step = longitudinal_df.shape[1] - 3\n    time_steps = 0\n    demographic_features = demographic_df.shape[1] - 1\n    \n    # train longitudinal data\n    lon_train_data_list = []\n    ###############################\n    # train demographics data\n    dem_train_data_list = []\n    ###############################\n    #test longitudinal data\n    lon_test_data_list = []\n    ###################################\n    #test demographic data\n    dem_test_data_list = []\n    ###################################\n    train_label_list = []\n    ###################################\n    test_label_list = []\n    \n    train = []\n    test = []\n    for key in longitudinal_train_data:\n        train.append(longitudinal_train_data[key])\n    for key in longitudinal_test_data:\n        test.append(longitudinal_test_data[key])\n        \n    X_train, y_train, demographic_train_data = create_dataset_train(train, number_of_training_visits,number_of_future_visits,\n                                                                    demographic_df)\n    # train data\n    lon_train_data_list.append(X_train)\n    dem_train_data_list.append(demographic_train_data)\n    train_label_list.append(y_train)\n    \n    X_test, y_test, demographic_test_data = create_dataset_test(test, number_of_training_visits, number_of_future_visits,\n                                                                demographic_df)\n    # test data\n    lon_test_data_list.append(X_test)\n    dem_test_data_list.append(demographic_test_data)\n    test_label_list.append(y_test)\n        \n    f = open('longitudinal_data_train.pkl', 'wb')\n    pickle.dump(lon_train_data_list, f)\n    f.close()\n    f = open('label_train.pkl', 'wb')\n    pickle.dump(train_label_list, f)\n    f.close()\n    f = open('demographic_data_train.pkl', 'wb')\n    pickle.dump(dem_train_data_list, f)\n    f.close()\n    f = open('longitudinal_data_test.pkl', 'wb')\n    pickle.dump(lon_test_data_list, f)\n    f.close()\n    f = open('label_test.pkl', 'wb')\n    pickle.dump(test_label_list, f)\n    f.close()\n    f = open('demographic_data_test.pkl', 'wb')\n    pickle.dump(dem_test_data_list, f)\n    f.close()\n    return 0   \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.498953Z","iopub.execute_input":"2023-04-19T03:39:24.499456Z","iopub.status.idle":"2023-04-19T03:39:24.520578Z","shell.execute_reply.started":"2023-04-19T03:39:24.499418Z","shell.execute_reply":"2023-04-19T03:39:24.519509Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# To call helping functions and generate pkl files.","metadata":{}},{"cell_type":"code","source":"if pkl_files_creator() == -1:\n    print('There is an error! Please run it again.')\nelse:\n    print('Data is ready as pkl files.')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:39:24.522017Z","iopub.execute_input":"2023-04-19T03:39:24.522322Z","iopub.status.idle":"2023-04-19T03:40:55.233111Z","shell.execute_reply.started":"2023-04-19T03:39:24.522292Z","shell.execute_reply":"2023-04-19T03:40:55.231380Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdin","text":"Please enter number of visits that you want to use for training the model:\n 2\nPlease enter number of future visits that you want to predict:\n 2\n"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2079294343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mpkl_files_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There is an error! Please run it again.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is ready as pkl files.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3237869288.py\u001b[0m in \u001b[0;36mpkl_files_creator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     X_test, y_test, demographic_test_data = create_dataset_test(test, number_of_training_visits, number_of_future_visits,\n\u001b[0;32m---> 94\u001b[0;31m                                                                 demographic_df)\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;31m# test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mlon_test_data_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/1201729273.py\u001b[0m in \u001b[0;36mcreate_dataset_test\u001b[0;34m(test_data_list, ts, fts, demographic_df)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Test data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mTestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT_num_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features_in_each_time_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtarget_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT_num_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 438 into shape (438,2,1)"],"ename":"ValueError","evalue":"cannot reshape array of size 438 into shape (438,2,1)","output_type":"error"}]}]}