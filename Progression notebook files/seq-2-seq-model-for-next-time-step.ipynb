{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Progression of Alzheimer's Disease (PPAD)","metadata":{}},{"cell_type":"code","source":"!pip install keras-tcn","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:55:28.220363Z","iopub.execute_input":"2023-04-19T03:55:28.220895Z","iopub.status.idle":"2023-04-19T03:55:53.004703Z","shell.execute_reply.started":"2023-04-19T03:55:28.220843Z","shell.execute_reply":"2023-04-19T03:55:53.003443Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting keras-tcn\n  Downloading keras_tcn-3.5.0-py3-none-any.whl (13 kB)\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (0.19.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (2.11.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (1.21.6)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (2.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (3.3.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (23.1.21)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (15.0.6.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (0.29.0)\nRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (2.11.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (4.4.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (3.8.0)\nRequirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (2.11.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (1.51.1)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (0.2.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (1.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (23.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (0.4.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (1.16.0)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (2.11.2)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (1.6.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-tcn) (59.8.0)\nRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons->keras-tcn) (2.13.3)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow->keras-tcn) (0.38.4)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (1.35.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (2.28.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (1.8.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (3.4.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (2.2.3)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (0.4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (4.11.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (1.26.14)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (3.11.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-tcn) (3.2.2)\nInstalling collected packages: protobuf, keras-tcn\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 21.12.2 requires cupy-cuda115, which is not installed.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\ntfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\napache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-tcn-3.5.0 protobuf-3.19.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nfrom pandas import read_csv\nimport pandas as pd\nimport random\nfrom keras.models import Sequential, Model\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import fbeta_score,accuracy_score,f1_score,roc_auc_score\nfrom keras.regularizers import l1, l2\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dense, SimpleRNN, concatenate, Input, Flatten\nfrom keras.layers import GRU\nfrom keras.layers import Dropout \nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Masking\nimport tensorflow as tf\nimport keras\nimport pickle\nfrom tcn import tcn","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:55:53.007567Z","iopub.execute_input":"2023-04-19T03:55:53.007988Z","iopub.status.idle":"2023-04-19T03:56:00.681973Z","shell.execute_reply.started":"2023-04-19T03:55:53.007937Z","shell.execute_reply":"2023-04-19T03:56:00.680911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# A customized binary cross entropy loss function\n#ð¿ð‘œð‘ ð‘  = âˆ’1/ð‘ âˆ‘(ð›¼ âˆ™ (ð‘¦ âˆ™ ð‘™ð‘œð‘” ð‘¦â€²)) + ((1 âˆ’ ð›¼) âˆ™ (1 âˆ’ ð‘¦) âˆ™ ð‘™ð‘œð‘”(1 âˆ’ ð‘¦â€²))","metadata":{}},{"cell_type":"code","source":"# to panelize positive (converter) misclassification\ndef binary_cross_entropy(y, yhat):\n    alpha = 0.7\n    loss = -(tf.math.reduce_mean((alpha * y * tf.math.log(yhat + 1e-6)) + ((1.0- alpha) * (1 - y) * tf.math.log(1 - yhat + 1e-6)), axis=-1))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:56:00.683501Z","iopub.execute_input":"2023-04-19T03:56:00.685022Z","iopub.status.idle":"2023-04-19T03:56:00.691381Z","shell.execute_reply.started":"2023-04-19T03:56:00.684976Z","shell.execute_reply":"2023-04-19T03:56:00.690228Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# PPAD","metadata":{}},{"cell_type":"code","source":"# PPAD method that takes the follwing parametres:\n# cell: it represents the RNN cell will be used {'GRU', 'biGRU', 'LSTM', 'biLSTM'}\n# drout: it represents the drop out rate will be used {0, 0.1, 0.2, 0.3, 0.4, 0.5}\n# L2: it represents the L2 regularization {0.1, 0.001, 0.00001, 0.0000001}\n# ftp: it represents future time point to predict in PPAD its 1\n\ndef PPAD_with_demographic(cell, drout, L2, ftp):\n    batch_shape = (None, time_steps, num_features_in_each_time_step)\n    model = Sequential()\n    model.add(Masking(-1, batch_input_shape=batch_shape))\n    \n    if cell == 'biGRU':\n        model.add(Bidirectional(GRU(16, activity_regularizer=l2(L2), return_sequences=True, activation='relu')))\n        model.add(Dropout(drout))\n        model.add(Bidirectional(GRU(8, activity_regularizer=l2(L2), return_sequences=False, activation='relu')))\n        model.add(Dropout(drout, name='out'))\n    elif cell == 'TCN' :\n        model.add(tcn.TCN(16, activity_regularizer=l2(L2), return_sequences=True, dilations=[1, 2, 4], activation='relu'))\n        model.add(Dropout(drout))\n        model.add(tcn.TCN(8, activity_regularizer=l2(L2), return_sequences= False, dilations=[1, 2, 4], activation='relu'))\n        model.add(Dropout(drout, name='out'))\n    elif cell == 'biLSTM':\n        model.add(Bidirectional(LSTM(16, activity_regularizer=l2(L2), return_sequences=True, activation='relu')))\n        model.add(Dropout(drout))\n        model.add(Bidirectional(LSTM(8, activity_regularizer=l2(L2), return_sequences=False, activation='relu')))\n        model.add(Dropout(drout, name='out'))\n    elif cell == 'GRU':\n        model.add(GRU(16, activity_regularizer=l2(L2), return_sequences=True, activation='relu'))\n        model.add(Dropout(drout))\n        model.add(GRU(8, activity_regularizer=l2(L2), return_sequences=False, activation='relu'))\n        model.add(Dropout(drout, name='out'))\n    elif cell == 'LSTM':\n        model.add(LSTM(16, activity_regularizer=l2(L2), return_sequences=True, activation='relu'))\n        model.add(Dropout(drout))\n        model.add(LSTM(8, activity_regularizer=l2(L2), return_sequences=False, activation='relu'))\n        model.add(Dropout(drout, name='out'))\n    \n    #Demographic model\n    model2 = Input(shape=(demographic_features))\n\n    # concatenating RNN output with demographic data\n    concat = concatenate([model.get_layer(name='out').output, model2], name='Concatenate')\n\n    # MLP Classification model    \n    final_model0 = Dense(8, activation='relu')(concat)\n    final_model1 = Dense(4, activation='relu')(final_model0)\n    final_model2 = Dense(1, activation='sigmoid')(final_model1)\n    final_model = Model(inputs=[model.inputs, model2], outputs=final_model2, name='Final_output')\n\n    final_model.compile(loss= binary_cross_entropy, optimizer='adam',metrics=['accuracy'])\n    return final_model\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:56:00.694698Z","iopub.execute_input":"2023-04-19T03:56:00.695491Z","iopub.status.idle":"2023-04-19T03:56:00.711565Z","shell.execute_reply.started":"2023-04-19T03:56:00.695452Z","shell.execute_reply":"2023-04-19T03:56:00.710547Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# F2 Calculation","metadata":{}},{"cell_type":"code","source":"# fbeata_function method to calculate f2 score\ndef overall_fbeta_function(pred, actual):\n    # reshape the output\n    if len(actual.shape) > 2:\n        actual = np.reshape(actual, (actual.shape[0], actual.shape[1]*actual.shape[2]))\n    \n    y = []\n    \n    for i in range(pred.shape[0]):\n        for j in range(pred.shape[1]):\n            if pred[i,j] > 0.5:\n                pred[i,j] = 1\n            else:\n                pred[i,j] = 0 \n    \n    for i in range(pred.shape[0]):\n        COUNTER = 0\n        while (COUNTER < actual.shape[1]):\n            if actual[i,COUNTER] != -1:\n                COUNTER+=1\n            else:\n                break\n        y.append(actual[i,COUNTER-1])\n    y = np.array(y) \n    y = np.reshape(y, (y.shape[0], 1))\n    \n    return fbeta_score(y, pred, beta=2)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T03:56:00.713020Z","iopub.execute_input":"2023-04-19T03:56:00.713470Z","iopub.status.idle":"2023-04-19T03:56:00.725282Z","shell.execute_reply.started":"2023-04-19T03:56:00.713429Z","shell.execute_reply":"2023-04-19T03:56:00.724072Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Function to build PPAD and train it using training data and evaluate it using test data.","metadata":{}},{"cell_type":"code","source":"# train and evaluate model. it returns a dataframe with training and test results\ndef do_PPAD(longitudinal_train_data, train_label, longitudinal_test_data, test_label, demographic_train_data,\n               demographic_test_data, iteration, ftp, hp_list):\n    X_train = longitudinal_train_data\n    y_train = train_label[:,-1]\n\n    X_test = longitudinal_test_data\n    y_test = test_label[:,-1]\n    \n    # hp\n    batch_size_ = int(hp_list[0])\n    epochs_ = int(hp_list[1])\n    drout = hp_list[2]\n    L2 = hp_list[3]\n    cell = \"biGRU\"\n    \n    print(cell)\n    \n    model = PPAD_with_demographic(cell, drout, L2, ftp)\n    history = model.fit([X_train, demographic_train_data], y_train, epochs=epochs_, batch_size = batch_size_,\n                        shuffle = True, verbose=0)\n    \n    #train\n    train_loss, train_acc = model.evaluate([X_train, demographic_train_data], y_train, batch_size = batch_size_, verbose=0)\n    train_pred = model.predict([X_train, demographic_train_data], verbose=0)\n    print('Training is over')\n    \n    #test\n    test_loss, test_acc = model.evaluate([X_test, demographic_test_data], y_test, batch_size = batch_size_, verbose=0)\n    test_pred = model.predict([X_test, demographic_test_data], verbose=0)\n    print('Test is over')\n    \n    # prepare results\n\n    for i in range(test_pred.shape[0]):\n        for j in range(test_pred.shape[1]):\n            if test_pred[i,j] > 0.5:\n                test_pred[i,j] = 1\n            else:\n                test_pred[i,j] = 0\n    \n\n    predicted_l = np.zeros((len(test_pred)))\n    real_l = np.zeros((len(y_test)))\n    dx = test_pred.shape[1] - 1\n    for i in range(len(test_pred)):\n        predicted_l[i] = test_pred[i,dx]\n    for i in range(len(y_test)):\n        real_l[i] = y_test[i,dx]\n    \n    print(\"predicted_l\")\n    print(predicted_l)\n    print(\"=====================================\")\n    \n    print(\"real_l\")\n    print(real_l)\n    print(\"=====================================\")\n    \n    \n    CM = confusion_matrix(real_l, predicted_l, labels=[0,1])\n    \n    print(\"CM\")\n    print(CM)\n    print(\"=====================================\")\n    \n    sensitivity = CM[1,1] / (CM[1,1] + CM[1,0])\n    specificity = CM[0,0] / (CM[0,0] + CM[0,1])\n    \n    F1_Score = f1_score(real_l, predicted_l, average='binary')\n    \n    print(\"F1_Score\")\n    print(F1_Score)\n    print(\"=====================================\")\n    \n    # Table of results\n    col = 'Iteration '+str(iteration)\n    metrics_results_df = pd.DataFrame(columns = [col])\n\n    metrics_results_df.loc[len(metrics_results_df)] = [round(accuracy_score(y_test[:, -1], test_pred[:, -1]), 3)]\n    metrics_results_df.loc[len(metrics_results_df)] = [round(test_loss, 3)]\n    metrics_results_df.loc[len(metrics_results_df)] = [round(roc_auc_score(y_test[:, -1], test_pred[:, -1]), 3)]\n    metrics_results_df.loc[len(metrics_results_df)] = [round(fbeta_score(y_test[:, -1], test_pred[:, -1], beta=2), 3)] \n    metrics_results_df.loc[len(metrics_results_df)] = [round(sensitivity, 3)] \n    metrics_results_df.loc[len(metrics_results_df)] = [round(specificity, 3)]  \n    metrics_results_df.loc[len(metrics_results_df)] = [round(train_acc, 3)]\n    metrics_results_df.loc[len(metrics_results_df)] = [round(train_loss, 3)]\n    metrics_results_df.loc[len(metrics_results_df)] = [round(overall_fbeta_function(train_pred, y_train), 3)]\n    \n    return metrics_results_df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:52.467920Z","iopub.execute_input":"2023-04-19T05:15:52.468336Z","iopub.status.idle":"2023-04-19T05:15:52.487923Z","shell.execute_reply.started":"2023-04-19T05:15:52.468299Z","shell.execute_reply":"2023-04-19T05:15:52.486829Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Method to create and return an empty dataframe for results \ndef create_table():\n    # Table of results\n    PPAD_metrics_results_df = pd.DataFrame(columns = ['Metrics'])\n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['Accuracy (Test)']\n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['Loss (Test)']\n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['ROC_AUC (Test)']\n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['F-2 (Test)'] \n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['Sensitivity (Test)'] \n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['Specificity (Test)']  \n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['Accuracy (Train)']\n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['Loss (Train)']\n    PPAD_metrics_results_df.loc[len(PPAD_metrics_results_df)] = ['F-2 (Train)']\n    \n    return PPAD_metrics_results_df ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:53.285878Z","iopub.execute_input":"2023-04-19T05:15:53.286549Z","iopub.status.idle":"2023-04-19T05:15:53.293870Z","shell.execute_reply.started":"2023-04-19T05:15:53.286510Z","shell.execute_reply":"2023-04-19T05:15:53.292702Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Best hyperparameters that have been chosen by grid search optimization","metadata":{}},{"cell_type":"code","source":"pwd()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:54.380099Z","iopub.execute_input":"2023-04-19T05:15:54.381292Z","iopub.status.idle":"2023-04-19T05:15:54.388706Z","shell.execute_reply.started":"2023-04-19T05:15:54.381244Z","shell.execute_reply":"2023-04-19T05:15:54.387479Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"# To read a csv file that contains best hyperparameters and copy it in a dataframe\n# Hyperparameters df contains the best values of batch_size, epoch, dropout, l2, and RNN cell\nfile_name = '/kaggle/input/hp-dp-new/hp_df_new.csv'\nPPAD_hp_df = read_csv(file_name,header=0)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:54.811735Z","iopub.execute_input":"2023-04-19T05:15:54.812100Z","iopub.status.idle":"2023-04-19T05:15:54.823822Z","shell.execute_reply.started":"2023-04-19T05:15:54.812066Z","shell.execute_reply":"2023-04-19T05:15:54.822837Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"PPAD_hp_df","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:55.222697Z","iopub.execute_input":"2023-04-19T05:15:55.223044Z","iopub.status.idle":"2023-04-19T05:15:55.235677Z","shell.execute_reply.started":"2023-04-19T05:15:55.223011Z","shell.execute_reply":"2023-04-19T05:15:55.233662Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"   batch_size  epoch  dropout     l2 cell\n0           8     50      0.4  0.001  TCN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>batch_size</th>\n      <th>epoch</th>\n      <th>dropout</th>\n      <th>l2</th>\n      <th>cell</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>50</td>\n      <td>0.4</td>\n      <td>0.001</td>\n      <td>TCN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Global variables","metadata":{}},{"cell_type":"code","source":"# unpikle data\n\n# Longitudinal training data\nfile_name = '/kaggle/input/our-ppad/longitudinal_data_train.pkl'\nlon_data_train = pd.read_pickle(file_name)\n\n# Labels of traing data \nfile_name = '/kaggle/input/our-ppad/label_train.pkl'\nlabel_train = pd.read_pickle(file_name)\n\n# Demographic training data\nfile_name = '/kaggle/input/our-ppad/demographic_data_train.pkl'\ndem_data_train = pd.read_pickle(file_name)\n\n# Longitudinal test data\nfile_name = '/kaggle/input/our-ppad/longitudinal_data_test.pkl'\nlon_data_test = pd.read_pickle(file_name)\n\n# Labels of test data \nfile_name = '/kaggle/input/our-ppad/label_test.pkl'\nlabel_test = pd.read_pickle(file_name)\n\n# Demographic test data\nfile_name = '/kaggle/input/our-ppad/demographic_data_test.pkl'\ndem_data_test = pd.read_pickle(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:55.984102Z","iopub.execute_input":"2023-04-19T05:15:55.984792Z","iopub.status.idle":"2023-04-19T05:15:55.998723Z","shell.execute_reply.started":"2023-04-19T05:15:55.984753Z","shell.execute_reply":"2023-04-19T05:15:55.997620Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# This represents number of visits (time points) will be used in the training.\ntime_steps = lon_data_test[0].shape[1]\n\n# This represents number of future visits ahead to predict \nfuture_time_s = label_test[0].shape[1]\n\n# This represents how many featutes in each visit (longitudinal).\nnum_features_in_each_time_step = lon_data_test[0].shape[2]\n\n# This represents how many demographic featutes (cross sectional).\ndemographic_features = len(dem_data_test[0][0])","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:56.546501Z","iopub.execute_input":"2023-04-19T05:15:56.546878Z","iopub.status.idle":"2023-04-19T05:15:56.553025Z","shell.execute_reply.started":"2023-04-19T05:15:56.546844Z","shell.execute_reply":"2023-04-19T05:15:56.551825Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"future_time_s","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:57.162944Z","iopub.execute_input":"2023-04-19T05:15:57.163623Z","iopub.status.idle":"2023-04-19T05:15:57.171010Z","shell.execute_reply.started":"2023-04-19T05:15:57.163585Z","shell.execute_reply":"2023-04-19T05:15:57.169802Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"markdown","source":"# Runing PPAD 5 times for one scenario and save results","metadata":{}},{"cell_type":"code","source":"if future_time_s == 1:\n    longitudinal_train_data = lon_data_train[0]\n    train_label = label_train[0]\n    longitudinal_test_data = lon_data_test[0]\n    test_label = label_test[0]\n    demographic_train_data = np.array(dem_data_train[0])\n    demographic_test_data = np.array(dem_data_test[0])\n\n    # HP\n    PPAD_hp_list = list(PPAD_hp_df.iloc[0,:])\n\n    PPAD_metrics_results_df = create_table()\n    for j in range(5):\n        print(\"iteration_\", j+1)\n        #PPAD\n        PPAD_result = do_PPAD(longitudinal_train_data, train_label, longitudinal_test_data, test_label, demographic_train_data,\n                              demographic_test_data, j+1, future_time_s, PPAD_hp_list)\n        PPAD_metrics_results_df = pd.concat([PPAD_metrics_results_df, PPAD_result], axis=1)\n        print(\"PPAD\")\n    # SAVE RESULTS\n    PPAD_scenario = str(time_steps)+'_'+str(future_time_s)+'_PPAD.csv'\n    PPAD_metrics_results_df.to_csv(PPAD_scenario, index = False)\nelse:\n    print('Number of future visit for prediction should be 1 for PPAD')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:15:58.542519Z","iopub.execute_input":"2023-04-19T05:15:58.543217Z","iopub.status.idle":"2023-04-19T05:33:22.908403Z","shell.execute_reply.started":"2023-04-19T05:15:58.543174Z","shell.execute_reply":"2023-04-19T05:33:22.907167Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"iteration_ 1\nbiGRU\nTraining is over\nTest is over\npredicted_l\n[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0.\n 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1.\n 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.\n 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0.]\n=====================================\nreal_l\n[0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n=====================================\nCM\n[[171  45]\n [ 25  94]]\n=====================================\nF1_Score\n0.7286821705426356\n=====================================\nPPAD\niteration_ 2\nbiGRU\nTraining is over\nTest is over\npredicted_l\n[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.\n 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0.]\n=====================================\nreal_l\n[0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n=====================================\nCM\n[[157  59]\n [ 14 105]]\n=====================================\nF1_Score\n0.7420494699646644\n=====================================\nPPAD\niteration_ 3\nbiGRU\nTraining is over\nTest is over\npredicted_l\n[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0.\n 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.\n 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n=====================================\nreal_l\n[0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n=====================================\nCM\n[[167  49]\n [ 25  94]]\n=====================================\nF1_Score\n0.7175572519083969\n=====================================\nPPAD\niteration_ 4\nbiGRU\nTraining is over\nTest is over\npredicted_l\n[0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1.\n 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.\n 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0.]\n=====================================\nreal_l\n[0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n=====================================\nCM\n[[174  42]\n [ 27  92]]\n=====================================\nF1_Score\n0.7272727272727273\n=====================================\nPPAD\niteration_ 5\nbiGRU\nTraining is over\nTest is over\npredicted_l\n[0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.\n 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0.]\n=====================================\nreal_l\n[0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1.\n 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n=====================================\nCM\n[[149  67]\n [ 10 109]]\n=====================================\nF1_Score\n0.7389830508474575\n=====================================\nPPAD\n","output_type":"stream"}]},{"cell_type":"code","source":"PPAD_result","metadata":{"execution":{"iopub.status.busy":"2023-04-19T05:33:22.910548Z","iopub.execute_input":"2023-04-19T05:33:22.910849Z","iopub.status.idle":"2023-04-19T05:33:22.921015Z","shell.execute_reply.started":"2023-04-19T05:33:22.910820Z","shell.execute_reply":"2023-04-19T05:33:22.919743Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"   Iteration 5\n0        0.770\n1        0.204\n2        0.803\n3        0.836\n4        0.916\n5        0.690\n6        0.804\n7        0.182\n8        0.847","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Iteration 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.770</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.204</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.803</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.836</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.916</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.690</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.804</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.182</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.847</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}