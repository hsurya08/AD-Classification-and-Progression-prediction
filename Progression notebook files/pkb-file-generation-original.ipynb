{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary lib\nimport numpy as np\nfrom pandas import read_csv\nimport pandas as pd\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nimport pickle\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.188517Z","iopub.execute_input":"2023-04-17T23:52:21.189623Z","iopub.status.idle":"2023-04-17T23:52:21.195925Z","shell.execute_reply.started":"2023-04-17T23:52:21.189573Z","shell.execute_reply":"2023-04-17T23:52:21.194582Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Helping functions","metadata":{}},{"cell_type":"code","source":"# Function to determine unique values in a dataframe's column based on the column's name.\ndef unique_value_function(df, feature_name):\n    if feature_name in df.columns:\n        _values = df[[feature_name]]\n        unique_values = _values.values\n        unique_values = np.unique(unique_values)\n        num_of_unique_values = np.unique(unique_values).shape[0]\n        #print('Number of unique values in '+feature_name+' is: ', num_of_unique_values)\n        return unique_values\n    else:\n        print(str(feature_name)+' is not a feature name in this dataframe.')\n        return -1","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.198079Z","iopub.execute_input":"2023-04-17T23:52:21.198721Z","iopub.status.idle":"2023-04-17T23:52:21.208121Z","shell.execute_reply.started":"2023-04-17T23:52:21.198679Z","shell.execute_reply":"2023-04-17T23:52:21.207262Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Function to check if the dataset has all necessary features and been preprocessed carefully.\ndef check_longitudinal_dataset(df):\n    # chech if the dataframe is empty\n    if df.empty:\n        print('Dataset is empty.')\n        return -1\n    \n    # Check for missing values\n    if df.isnull().sum().any():\n        print('Dataset has NAN values and needs to preprocess.')\n        return -1\n    \n    # Check for the existence of important features\n    features_name = df.columns\n    if not('RID' in features_name) or not('VISCODE' in features_name) or not('DX' in features_name):\n        print('Dataset does not have necessary feature/s')\n        return -1\n    \n    # chech if the RID's is numeric\n    if not(is_numeric_dtype(df['RID'])):\n        print('Patient ID should be numeric.')\n        return -1\n    \n    # Check for the existence of only Dementia and MCI as diagnosis\n    unique_diagnosis = unique_value_function(df, 'DX')\n    if (len(unique_diagnosis) != 2) or not('Dementia' in unique_diagnosis) or not('MCI' in unique_diagnosis):\n        print('Dataset does not have correct diagnosis or unique number of diagnosis.')\n        return -1\n    \n    # Check if the dataframe has at least one longitudinal feature other than RID, VISCODE, and DX\n    if not(df.shape[1] > 3):\n        print('Dataset does not have enough features')\n        return -1\n    \n    # Check if the longitudinal data is numiric\n    features_name = df.columns\n    flag = False\n    for i in range(len(features_name)):\n        if not(features_name[i] in ['RID', 'DX', 'VISCODE']):\n            if not(is_numeric_dtype(df[features_name[i]])):\n                flag = True\n    if flag:\n        print('Data should be numeric.')\n        return -1\n            \n    \n    # return 1 if the dataset is ready\n    return 1","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.209660Z","iopub.execute_input":"2023-04-17T23:52:21.210004Z","iopub.status.idle":"2023-04-17T23:52:21.222957Z","shell.execute_reply.started":"2023-04-17T23:52:21.209970Z","shell.execute_reply":"2023-04-17T23:52:21.221949Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Function to check if the dataset has all necessary features and been preprocessed carefully.\ndef check_demographic_dataset(df):\n    # chech if the dataframe is empty\n    if df.empty:\n        print('Dataset is empty.')\n        return -1\n    \n    # Check for missing values\n    if df.isnull().sum().any():\n        print('Dataset has NAN values and needs to preprocess.')\n        return -1\n    \n    # Check for the existence of important features\n    features_name = df.columns\n    if not('RID' in features_name):\n        print('Dataset does not have necessary feature/s')\n        return -1\n    \n    # chech if the RID's is numeric\n    if not(is_numeric_dtype(df['RID'])):\n        print('Patient ID should be numeric.')\n        return -1\n    \n    # Check if the dataframe has at least one demographic feature other than RID\n    if not(df.shape[1] > 1):\n        print('Dataset does not have enough features')\n        return -1\n    \n    # return 1 if the dataset is ready\n    return 1","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.225028Z","iopub.execute_input":"2023-04-17T23:52:21.225644Z","iopub.status.idle":"2023-04-17T23:52:21.239588Z","shell.execute_reply.started":"2023-04-17T23:52:21.225606Z","shell.execute_reply":"2023-04-17T23:52:21.238707Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Function to prepare longitudinal data (VISCODE)\ndef visit_code_preperation(df):\n    unique_visitcode = unique_value_function(df, 'VISCODE')\n    unique_id = unique_value_function(df, 'RID')\n    \n    columns_name = list(df.columns)\n    new_df = pd.DataFrame(columns = columns_name)\n    \n    for i in range(len(unique_id)):\n        temp_data = df[df[\"RID\"] == unique_id[i]]\n        temp_data.reset_index(drop=True,inplace=True)\n        size = temp_data.shape[0]\n        \n        for j in range(size):\n            temp_data.loc[j, 'VISCODE'] = j*6\n            new_row = temp_data.iloc[j,:]\n            new_df.loc[len(new_df)] = new_row\n    \n    return new_df","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.240921Z","iopub.execute_input":"2023-04-17T23:52:21.241249Z","iopub.status.idle":"2023-04-17T23:52:21.255581Z","shell.execute_reply.started":"2023-04-17T23:52:21.241217Z","shell.execute_reply":"2023-04-17T23:52:21.254202Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def num_patients_visits_function(df, _list):\n    # Calculate maximum number of visits\n    visit_size = 0\n    for i in range(len(_list)):\n        temp_data = df[df[\"RID\"] == _list[i]]\n        size = (temp_data.shape)[0]\n        if size > visit_size:\n            visit_size = size\n\n    # Calculate how many patients in each number of visit groups for removed patients\n    visit_size = np.zeros((visit_size),int)\n    for i in range(len(_list)):\n        temp_data = df[df[\"RID\"] == _list[i]]\n        size = (temp_data.shape)[0]\n        visit_size[size-1] = visit_size[size-1] + 1\n\n    for i in range(len(visit_size)):\n        print (i+1,'_Visit = ', visit_size[i])","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.257029Z","iopub.execute_input":"2023-04-17T23:52:21.258152Z","iopub.status.idle":"2023-04-17T23:52:21.266736Z","shell.execute_reply.started":"2023-04-17T23:52:21.258100Z","shell.execute_reply":"2023-04-17T23:52:21.265583Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Encoding Diagnosis DX (MCI = 0 and Dementia = 1)\ndef encode_diagnosis(df):\n    for i in range(len(df)):\n        if df.loc[i, 'DX'] == 'MCI':\n            df.loc[i, 'DX'] = 0\n        else:\n            df.loc[i, 'DX'] = 1\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.267935Z","iopub.execute_input":"2023-04-17T23:52:21.268268Z","iopub.status.idle":"2023-04-17T23:52:21.280496Z","shell.execute_reply.started":"2023-04-17T23:52:21.268235Z","shell.execute_reply":"2023-04-17T23:52:21.279472Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Normalize longitudinal data using min-max normalization\ndef min_max_normalization(df):\n    columns = list(df.columns)\n    new_arrangment_for_columns = ['RID','VISCODE']\n    for col in columns:\n        if not(col in new_arrangment_for_columns) and col != 'DX':\n            new_arrangment_for_columns.append(col)\n    new_arrangment_for_columns.append('DX')\n    \n    df = df[new_arrangment_for_columns]\n    \n    for i in range(2, len(new_arrangment_for_columns)-1):   \n        temp_data = df.iloc[:,i]\n        max_value = temp_data.max()\n        min_value = temp_data.min()\n        for j in range(len(df)):\n            df.iat[j, i] = (df.iloc[j, i]-min_value)/(max_value - min_value)\n            \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.372641Z","iopub.execute_input":"2023-04-17T23:52:21.373540Z","iopub.status.idle":"2023-04-17T23:52:21.382660Z","shell.execute_reply.started":"2023-04-17T23:52:21.373499Z","shell.execute_reply":"2023-04-17T23:52:21.380875Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Function to group patients together based on number of visits they have\ndef group_patients_according_number_of_visits(df):\n    unique_id = unique_value_function(df, 'RID')\n    visits_dic = {}\n    \n    for i in range(len(unique_id)):\n        temp_data = df[df[\"RID\"] == unique_id[i]]\n        temp_data.reset_index(drop=True,inplace=True)\n        size = temp_data.shape[0]\n        \n        if size in visits_dic:\n            visits_dic[size] = pd.concat([visits_dic[size], temp_data])\n            visits_dic[size].reset_index(drop=True, inplace=True)\n        else:\n            visits_dic[size] = temp_data\n            \n    # sort the dictionary based on the key\n    sorted_dic = {}\n    for key in sorted(visits_dic):\n        sorted_dic[key] = visits_dic[key]\n    return sorted_dic","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.385250Z","iopub.execute_input":"2023-04-17T23:52:21.385830Z","iopub.status.idle":"2023-04-17T23:52:21.396632Z","shell.execute_reply.started":"2023-04-17T23:52:21.385779Z","shell.execute_reply":"2023-04-17T23:52:21.395495Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Function to transpose the longitudinal dataset\ndef transpose_longitudinal_data(group_longitudinal_data_dic, features_to_be_in_columns):\n    transposed_lonitudinal_data_dic = {}\n    for key in (group_longitudinal_data_dic):\n        transposed_lonitudinal_data_dic[key] = group_longitudinal_data_dic[key].pivot(index = 'RID', columns= 'VISCODE',\n                                                                                      values= features_to_be_in_columns)\n        \n    new_columns_names_dic = {}\n    for key in (group_longitudinal_data_dic):\n        new_columns_names_dic[key] = ['RID']\n    for key in (group_longitudinal_data_dic):\n        time_points = key\n        \n        for i in range(time_points):\n            for j in range(1, int(len(transposed_lonitudinal_data_dic[key].columns)/time_points+1)):\n                column_idex = i + (key * j) - key\n                new_columns_names_dic[key].append(transposed_lonitudinal_data_dic[key].columns[column_idex][0] + '_'+ str(transposed_lonitudinal_data_dic[key].columns[column_idex][1]))\n                \n    final_longitudinal_data_dic = {}\n    for key in (group_longitudinal_data_dic):\n        time_points = key\n        unique_rid = unique_value_function(group_longitudinal_data_dic[key], 'RID')\n\n        final_longitudinal_data_dic[key] = pd.DataFrame(columns = new_columns_names_dic[key])\n        for x in range(len(transposed_lonitudinal_data_dic[key])):\n            new_time_point_data = []\n            new_time_point_data.append(unique_rid[x])\n            for i in range(time_points):\n                for j in range(1, int(len(transposed_lonitudinal_data_dic[key].columns)/time_points+1)):\n                    column_idex = i + (time_points * j) - time_points\n                    new_time_point_data.append(transposed_lonitudinal_data_dic[key].iloc[x, column_idex])\n            final_longitudinal_data_dic[key].loc[len(final_longitudinal_data_dic[key])] = new_time_point_data\n            \n    return final_longitudinal_data_dic","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.398218Z","iopub.execute_input":"2023-04-17T23:52:21.398583Z","iopub.status.idle":"2023-04-17T23:52:21.412302Z","shell.execute_reply.started":"2023-04-17T23:52:21.398542Z","shell.execute_reply":"2023-04-17T23:52:21.411348Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Apply one-hot-encoding only for categorical demographics features\ndef demographic_one_hot_encoding(demographic_df):\n    \n    demographic_data = demographic_df\n    categorical_columns = []\n    all_columns = list(demographic_data.columns)\n    for i in range(len(all_columns)):\n        if all_columns[i] != 'RID' and all_columns[i] != 'PTEDUCAT':\n            categorical_columns.append(all_columns[i])\n\n    for c in range(len(categorical_columns)):\n        tempdf = pd.get_dummies(demographic_data[categorical_columns[c]], prefix=categorical_columns[c])\n        demographic_data = pd.concat([demographic_data, tempdf], axis=1)\n        demographic_data = demographic_data.drop(columns=categorical_columns[c])\n\n    categorical_columns_will_be_used = list(demographic_data.columns)\n\n    temp_columns = demographic_data.columns\n    temp_keep_these_columns = []\n    for c in range(len(temp_columns)):\n        for k in range(len(categorical_columns_will_be_used)):\n            if categorical_columns_will_be_used[k] in temp_columns[c]:\n                temp_keep_these_columns.append(temp_columns[c])\n    demographic_data = demographic_data[temp_keep_these_columns]\n    \n    return demographic_data","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.414550Z","iopub.execute_input":"2023-04-17T23:52:21.415595Z","iopub.status.idle":"2023-04-17T23:52:21.426274Z","shell.execute_reply.started":"2023-04-17T23:52:21.415547Z","shell.execute_reply":"2023-04-17T23:52:21.425095Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Function to split the longitudinal data into training and test data 70% and 30% respectively\ndef split_longitudinal_data(longitudinal_data_dic):\n    train_data = {}\n    test_data = {}\n    \n    for key in longitudinal_data_dic:\n        print(key)\n        print(longitudinal_data_dic[key].shape)\n        X_train, X_test = train_test_split(longitudinal_data_dic[key], test_size=0.3, random_state=42)\n        print(X_train.shape, X_test.shape)\n        if key in train_data:\n            train_data[key] = pd.concat([train_data[key], X_train])\n            train_data[key].reset_index(drop=True, inplace=True)\n            \n            test_data[key] = pd.concat([test_data[key], X_test])\n            test_data[key].reset_index(drop=True, inplace=True)\n        else:\n            train_data[key] = X_train\n            test_data[key] = X_test\n            \n    return train_data, test_data","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.428167Z","iopub.execute_input":"2023-04-17T23:52:21.428610Z","iopub.status.idle":"2023-04-17T23:52:21.443839Z","shell.execute_reply.started":"2023-04-17T23:52:21.428566Z","shell.execute_reply":"2023-04-17T23:52:21.442607Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Global veriable\nnum_features_in_each_time_step = 1\ntime_steps = 2\ndemographic_features = 1\n\n# Training data lists\ndataset = []\ndemographic_train = []\ntarget_1 = []\n\n# Test data lists\nTestset = []\ndemographic_test = []\ntarget_2 = []","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.445841Z","iopub.execute_input":"2023-04-17T23:52:21.446588Z","iopub.status.idle":"2023-04-17T23:52:21.457570Z","shell.execute_reply.started":"2023-04-17T23:52:21.446541Z","shell.execute_reply":"2023-04-17T23:52:21.456420Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Function to create training lists (longitudinal, demographics, label)\ndef create_train_lists(longitudinal_df, demographic_df, tp, ftp):\n    global dataset\n    global demographic_train\n    global target_1\n    \n    uid = unique_value_function(longitudinal_df, 'RID')\n    temp_demographic_df = pd.DataFrame(columns = list(demographic_df.columns)[1::])\n    for i in range(len(uid)):\n        temp_data = demographic_df[demographic_df[\"RID\"] == uid[i]]\n        #print(\"My RID is : \", uid[i])\n        #print(\"I am there here: \", temp_data)\n        temp_data.reset_index(drop=True, inplace=True)\n        new_row = temp_data.iloc[0,1:]\n        #print(new_row)\n        temp_demographic_df.loc[len(temp_demographic_df)] = new_row\n    \n    \n    num_feature_in_tp = num_features_in_each_time_step\n    df1 = longitudinal_df[longitudinal_df.columns]\n    print(df1.shape)\n#     print(df1.head(4))\n    \n    print(list(longitudinal_df.columns))\n    diagnosis_columns_names = []\n    all_columns = list(longitudinal_df.columns)\n    for i in range(len(all_columns)):\n        if 'DX_' in all_columns[i]:\n            diagnosis_columns_names.append(all_columns[i])\n    print(\"Ula Ula Ula : \", diagnosis_columns_names)\n    print(\"againnnnn\")\n#     print(df1.shape)\n    # dataframe at least has one time point for data and ftp for prediction \n    print(df1.shape[1],num_feature_in_tp)\n    print((df1.shape[1] - 1) / (num_feature_in_tp + 1))\n    print(ftp+1)\n    if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= 1:\n        print(\"inside: dataframe at least has one time point for data and ftp for prediction\")\n        Features = df1.loc[:, ~df1.columns.isin(diagnosis_columns_names)]\n        \n        Labels = df1.loc[:, df1.columns.isin(diagnosis_columns_names)]\n\n        print(df1.shape, temp_demographic_df.shape, len(df1), len(temp_demographic_df))           \n        # dataframe has tp and ftp\n        if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= tp+ftp:\n            print(\"inside df has tp and ftp\")\n            for i in range(len(df1)):\n                dataset.append(list(Features.iloc[i,1:tp*num_feature_in_tp+1]))\n                demographic_train.append(list(temp_demographic_df.iloc[i,:]))\n                target_1.append(list(Labels.iloc[i,tp:tp+ftp]))\n        else:\n            for i in range(len(df1)):\n                dataset.append(list(Features.iloc[i,1:Features.shape[1] - (ftp*num_feature_in_tp)]))\n                demographic_train.append(list(temp_demographic_df.iloc[i,:]))\n                target_1.append(list(Labels.iloc[i,Labels.shape[1]-ftp:]))\n    print(\"inside create_train_lists\")\n    print(dataset)\n    print(\"--------------------------------------------------------\")\n    print(demographic_train)\n    print(\"--------------------------------------------------------\")\n    print(target_1)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T00:52:17.112473Z","iopub.execute_input":"2023-04-18T00:52:17.112874Z","iopub.status.idle":"2023-04-18T00:52:17.132208Z","shell.execute_reply.started":"2023-04-18T00:52:17.112838Z","shell.execute_reply":"2023-04-18T00:52:17.130644Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Function to create test lists (longitudinal, demographics, label)\ndef create_test_lists(longitudinal_df, demographic_df, tp, ftp):\n    global Testset\n    global target_2\n    global demographic_test\n    global target_2_prev\n    \n    uid = unique_value_function(longitudinal_df, 'RID')\n    temp_demographic_df = pd.DataFrame(columns = list(demographic_df.columns)[1::])\n    for i in range(len(uid)):\n        temp_data = demographic_df[demographic_df[\"RID\"] == uid[i]]\n        temp_data.reset_index(drop=True, inplace=True)\n        new_row = temp_data.iloc[0,1:]\n        temp_demographic_df.loc[len(temp_demographic_df)] = new_row\n    \n    \n    num_feature_in_tp = num_features_in_each_time_step\n    df1 = longitudinal_df[longitudinal_df.columns]\n    \n    diagnosis_columns_names = []\n    all_columns = list(longitudinal_df.columns)\n    for i in range(len(all_columns)):\n        if 'DX_' in all_columns[i]:\n            diagnosis_columns_names.append(all_columns[i])\n    \n    # dataframe must have tp+ftp visits \n    if (df1.shape[1] - 1) / (num_feature_in_tp + 1) >= tp+ftp:\n        Features = df1.loc[:, ~df1.columns.isin(diagnosis_columns_names)]\n        \n        Labels = df1.loc[:, df1.columns.isin(diagnosis_columns_names)]\n\n        for i in range(len(df1)):\n            Testset.append(list(Features.iloc[i,1:tp*num_feature_in_tp+1]))\n            demographic_test.append(list(temp_demographic_df.iloc[i,:]))\n            target_2.append(list(Labels.iloc[i,tp:tp+ftp]))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.479827Z","iopub.execute_input":"2023-04-17T23:52:21.480612Z","iopub.status.idle":"2023-04-17T23:52:21.494974Z","shell.execute_reply.started":"2023-04-17T23:52:21.480548Z","shell.execute_reply":"2023-04-17T23:52:21.493919Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Function to create the train dataset\ndef create_dataset_train(train_data_list, ts, fts, demographic_df):\n    global time_steps\n    global demographic_features\n    global dataset\n    global demographic_train\n    global target_1\n    \n    dataset = []\n    demographic_train = []\n    target_1 = []\n\n    time_steps = ts\n    \n    train_df_list = []\n    \n    for i in range(len(train_data_list)):\n        train_df_list.append(train_data_list[i])\n    \n    #create_train_lists(df1_train,time_steps)\n    for i in range(len(train_df_list)):\n        print(i,train_df_list[i].shape)\n        create_train_lists(train_df_list[i], demographic_df, time_steps, fts)\n        print(dataset)\n#         print(create_train_lists(train_df_list[i], demographic_df, time_steps, fts))\n    \n    print(\"after\")\n    \n    # Train Padding\n    padded1 = pad_sequences(dataset, padding='post',dtype='float', value=-1)\n    print(\"----------------------------------------------------\")\n    print(padded1)\n\n    num_samples = len(padded1)\n    num_features = padded1.shape[1]\n    time_steps = int(num_features / num_features_in_each_time_step)\n    dataset = padded1\n    padded_ = pad_sequences(target_1, padding='post',dtype='float', value=-1)\n    target_1 = padded_\n    num_labels = padded_.shape[1]\n    # data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n    dataset = np.reshape(dataset, (num_samples, time_steps, num_features_in_each_time_step))\n    target_1 = np.reshape(target_1, (num_samples, num_labels, 1))\n\n    \n    return dataset, target_1, demographic_train","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.496893Z","iopub.execute_input":"2023-04-17T23:52:21.497376Z","iopub.status.idle":"2023-04-17T23:52:21.511351Z","shell.execute_reply.started":"2023-04-17T23:52:21.497329Z","shell.execute_reply":"2023-04-17T23:52:21.510329Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Function to create test dataset\ndef create_dataset_test(test_data_list, ts, fts, demographic_df):\n    global time_steps\n    global demographic_features\n    global Testset\n    global target_2\n    global demographic_test\n    global target_2_prev\n    \n    Testset = []\n    target_2 = []\n    demographic_test = []\n    target_2_prev = []\n\n    time_steps = ts\n    \n    test_df_list = []\n    \n    for i in range(len(test_data_list)):\n        test_df_list.append(test_data_list[i])\n    \n    #create_train_lists(df1_train,time_steps)\n    for i in range(len(test_df_list)):\n        create_test_lists(test_df_list[i], demographic_df, time_steps, fts)\n        \n    # Test Padding\n    padded2 = pad_sequences(Testset, padding='post',dtype='float', value=-1)\n\n    T_num_samples = len(padded2)\n    Testset = padded2\n    target_2 = np.array(target_2)\n\n    # Test data and target are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features].\n    Testset = np.reshape(Testset, (T_num_samples, time_steps, num_features_in_each_time_step))\n    target_2 = np.reshape(target_2, (T_num_samples, fts, 1))\n\n    \n    return Testset, target_2, demographic_test","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.514329Z","iopub.execute_input":"2023-04-17T23:52:21.515306Z","iopub.status.idle":"2023-04-17T23:52:21.527164Z","shell.execute_reply.started":"2023-04-17T23:52:21.515263Z","shell.execute_reply":"2023-04-17T23:52:21.526239Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Main function\ndef pkl_files_creator():\n    global num_features_in_each_time_step\n    global time_steps\n    global demographic_features\n    # Read csv files for longitudinal and demographic data\n    # Longitudinal data\n    file_name = '/kaggle/input/dataset-17th-april/17thapril_longitudinal.csv'\n    longitudinal_df = read_csv(file_name, header=0)\n\n    # Demographic data\n    file_name = '/kaggle/input/dataset-17th-april/17thapril_demographic.csv'\n    demographic_df = read_csv(file_name, header=0)\n    \n    # working on longitudinal data\n    if check_longitudinal_dataset(longitudinal_df) == -1:\n        return -1\n    if check_demographic_dataset(demographic_df) == -1:\n        return -1\n    longitudinal_df = visit_code_preperation(longitudinal_df)\n    longitudinal_df = encode_diagnosis(longitudinal_df)\n    longitudinal_df = min_max_normalization(longitudinal_df)\n    print(longitudinal_df.shape)\n    longitudinal_df_dic = group_patients_according_number_of_visits(longitudinal_df)\n    \n    \n    features_to_be_in_columns = 0\n    print(\"features_to_be_in_columns\")\n    for key in longitudinal_df_dic:\n        features_to_be_in_columns = list(longitudinal_df_dic[key].columns)[2::]\n#         print(key, features_to_be_in_columns)\n        break\n#     longitudinal_df_dic = transpose_longitudinal_data(longitudinal_df_dic, features_to_be_in_columns)\n#     print(longitudinal_df_dic)\n    longitudinal_train_data, longitudinal_test_data = split_longitudinal_data(longitudinal_df_dic)\n    \n    # working on demographic data\n    demographic_df = demographic_one_hot_encoding(demographic_df)\n    \n    # user choises\n    number_of_training_visits = input(\"Please enter number of visits that you want to use for training the model:\\n\")\n    while not(number_of_training_visits.isdigit()):\n        number_of_training_visits = input(\"Please enter integer values:\\n\")\n    number_of_training_visits = int(number_of_training_visits)\n    \n    number_of_future_visits = input(\"Please enter number of future visits that you want to predict:\\n\")\n    while not(number_of_future_visits.isdigit()):\n        number_of_future_visits = input(\"Please enter integer values:\\n\")\n    number_of_future_visits = int(number_of_future_visits)\n    \n    key_list = []\n    for key in longitudinal_df_dic:\n        key_list.append(key)\n    minimum_visit = key_list[0]\n    maximum_visit = key_list[-1]\n    if (number_of_future_visits + number_of_training_visits) > maximum_visit:\n        print('The Dataset does not have enough visits for this selection')\n        return -1\n    if number_of_training_visits < minimum_visit or number_of_future_visits < 1:\n        print('Wrong selection')\n        return -1\n    \n    num_features_in_each_time_step = longitudinal_df.shape[1] - 3\n    time_steps = 0\n    demographic_features = demographic_df.shape[1] - 1\n    \n    # train longitudinal data\n    lon_train_data_list = []\n    ###############################\n    # train demographics data\n    dem_train_data_list = []\n    ###############################\n    #test longitudinal data\n    lon_test_data_list = []\n    ###################################\n    #test demographic data\n    dem_test_data_list = []\n    ###################################\n    train_label_list = []\n    ###################################\n    test_label_list = []\n    \n    train = []\n    test = []\n    for key in longitudinal_train_data:\n        train.append(longitudinal_train_data[key])\n    for key in longitudinal_test_data:\n        test.append(longitudinal_test_data[key])\n    \n#     print(\"train --------------------------------------------------\")\n#     print(train)\n#     print(\"test --------------------------------------------------\")\n#     print(test)\n        \n    X_train, y_train, demographic_train_data = create_dataset_train(train, number_of_training_visits,number_of_future_visits,\n                                                                    demographic_df)\n    # train data\n    lon_train_data_list.append(X_train)\n    dem_train_data_list.append(demographic_train_data)\n    train_label_list.append(y_train)\n    \n    X_test, y_test, demographic_test_data = create_dataset_test(test, number_of_training_visits, number_of_future_visits,\n                                                                demographic_df)\n    # test data\n    lon_test_data_list.append(X_test)\n    dem_test_data_list.append(demographic_test_data)\n    test_label_list.append(y_test)\n        \n    f = open('longitudinal_data_train.pkl', 'wb')\n    pickle.dump(lon_train_data_list, f)\n    f.close()\n    f = open('label_train.pkl', 'wb')\n    pickle.dump(train_label_list, f)\n    f.close()\n    f = open('demographic_data_train.pkl', 'wb')\n    pickle.dump(dem_train_data_list, f)\n    f.close()\n    f = open('longitudinal_data_test.pkl', 'wb')\n    pickle.dump(lon_test_data_list, f)\n    f.close()\n    f = open('label_test.pkl', 'wb')\n    pickle.dump(test_label_list, f)\n    f.close()\n    f = open('demographic_data_test.pkl', 'wb')\n    pickle.dump(dem_test_data_list, f)\n    f.close()\n    return 0   \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-17T23:52:21.528889Z","iopub.execute_input":"2023-04-17T23:52:21.529722Z","iopub.status.idle":"2023-04-17T23:52:21.550514Z","shell.execute_reply.started":"2023-04-17T23:52:21.529682Z","shell.execute_reply":"2023-04-17T23:52:21.549418Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# To call helping functions and generate pkl files.","metadata":{}},{"cell_type":"code","source":"if pkl_files_creator() == -1:\n    print('There is an error! Please run it again.')\nelse:\n    print('Data is ready as pkl files.')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T00:52:23.163558Z","iopub.execute_input":"2023-04-18T00:52:23.164000Z","iopub.status.idle":"2023-04-18T00:53:34.208374Z","shell.execute_reply.started":"2023-04-18T00:52:23.163959Z","shell.execute_reply":"2023-04-18T00:53:34.206541Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"(6590, 15)\nfeatures_to_be_in_columns\n1\n(221, 15)\n(154, 15) (67, 15)\n2\n(238, 15)\n(166, 15) (72, 15)\n3\n(600, 15)\n(420, 15) (180, 15)\n4\n(1080, 15)\n(756, 15) (324, 15)\n5\n(750, 15)\n(525, 15) (225, 15)\n6\n(1158, 15)\n(810, 15) (348, 15)\n7\n(791, 15)\n(553, 15) (238, 15)\n8\n(480, 15)\n(336, 15) (144, 15)\n9\n(459, 15)\n(321, 15) (138, 15)\n10\n(260, 15)\n(182, 15) (78, 15)\n11\n(132, 15)\n(92, 15) (40, 15)\n12\n(180, 15)\n(126, 15) (54, 15)\n13\n(156, 15)\n(109, 15) (47, 15)\n14\n(70, 15)\n(49, 15) (21, 15)\n15\n(15, 15)\n(10, 15) (5, 15)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Please enter number of visits that you want to use for training the model:\n 2\nPlease enter number of future visits that you want to predict:\n 1\n"},{"name":"stdout","text":"0 (154, 15)\n(154, 15)\n['RID', 'VISCODE', 'AGE', 'ADAS11', 'ADAS13', 'ADASQ4', 'MMSE', 'Ventricles', 'Hippocampus', 'WholeBrain', 'Entorhinal', 'Fusiform', 'MidTemp', 'ICV', 'DX']\nUla Ula Ula :  []\nagainnnnn\n15 12\n1.0769230769230769\n2\ninside: dataframe at least has one time point for data and ftp for prediction\n(154, 15) (154, 21) 154 154\ninside create_train_lists\n[[0, 0.5459459459459457], [0, 0.2567567567567567], [0, 0.31621621621621604], [0, 0.4], [0, 0.45945945945945954], [0, 0.5729729729729728], [0, 0.7243243243243243], [0, 0.6648648648648647], [0, 0.8054054054054054], [0, 0.5000000000000001], [0, 0.2243243243243244], [0, 0.6081081081081082], [0, 0.0702702702702703], [0, 0.5567567567567567], [0, 0.6432432432432432], [0, 0.843243243243243], [0, 0.6864864864864864], [0, 0.48378378378378367], [0, 0.4729729729729731], [0, 0.15945945945945939], [0, 0.3783783783783785], [0, 0.6729729729729728], [0, 0.22162162162162166], [0, 0.6999999999999998], [0, 0.8135135135135134], [0, 0.2378378378378379], [0, 0.4351351351351351], [0, 0.7567567567567568], [0, 0.5648648648648648], [0, 0.22702702702702696], [0, 0.1864864864864864], [0, 0.5297297297297296], [0, 0.8513513513513514], [0, 0.35405405405405405], [0, 0.4108108108108106], [0, 0.5270270270270271], [0, 0.6297297297297297], [0, 0.5135135135135136], [0, 0.7621621621621619], [0, 0.4486486486486486], [0, 0.808108108108108], [0, 0.5162162162162162], [0, 0.5729729729729728], [0, 0.4810810810810811], [0, 0.327027027027027], [0, 0.5864864864864863], [0, 0.7297297297297298], [0, 0.8648648648648649], [0, 0.4729729729729731], [0, 0.943243243243243], [0, 0.5783783783783782], [0, 0.1540540540540541], [0, 0.29189189189189196], [0, 0.3972972972972971], [0, 0.4], [0, 0.4513513513513511], [0, 0.46486486486486467], [0, 0.418918918918919], [0, 0.5945945945945946], [0, 0.4540540540540541], [0, 0.6432432432432432], [0, 0.33513513513513504], [0, 0.5189189189189187], [0, 0.6459459459459458], [0, 0.5162162162162162], [0, 0.5243243243243242], [0, 0.6270270270270268], [0, 0.43783783783783764], [0, 0.4918918918918917], [0, 0.47837837837837816], [0, 0.7945945945945944], [0, 0.7459459459459459], [0, 0.924324324324324], [0, 0.29459459459459447], [0, 0.8675675675675675], [0, 0.5351351351351351], [0, 0.6189189189189188], [0, 0.2486486486486487], [0, 0.20270270270270266], [0, 0.7756756756756754], [0, 0.7567567567567568], [0, 0.34324324324324307], [0, 0.6378378378378378], [0, 0.6081081081081082], [0, 0.9027027027027025], [0, 0.28648648648648645], [0, 0.6027027027027027], [0, 0.1864864864864864], [0, 0.15675675675675685], [0, 0.6054054054054052], [0, 0.418918918918919], [0, 0.28378378378378394], [0, 0.2702702702702704], [0, 0.5405405405405407], [0, 0.5999999999999998], [0, 0.7621621621621619], [0, 0.12972972972972982], [0, 0.5027027027027027], [0, 0.6243243243243243], [0, 0.6675675675675673], [0, 0.5594594594594592], [0, 0.7432432432432433], [0, 0.5405405405405407], [0, 0.5567567567567567], [0, 0.02432432432432428], [0, 0.26486486486486494], [0, 0.1918918918918919], [0, 0.032432432432432504], [0, 0.8027027027027024], [0, 0.6945945945945944], [0, 0.7432432432432433], [0, 0.7756756756756754], [0, 0.42432432432432415], [0, 0.04324324324324327], [0, 0.47027027027027013], [0, 0.26486486486486494], [0, 0.327027027027027], [0, 0.3324324324324325], [0, 0.4540540540540541], [0, 0.7729729729729728], [0, 0.2297297297297297], [0, 0.7837837837837838], [0, 0.5432432432432431], [0, 0.935135135135135], [0, 0.6783783783783783], [0, 0.8243243243243243], [0, 0.040540540540540536], [0, 0.47027027027027013], [0, 0.5459459459459457], [0, 0.10810810810810809], [0, 0.6837837837837838], [0, 0.05675675675675679], [0, 0.7135135135135133], [0, 0.7783783783783783], [0, 0.09189189189189184], [0, 0.6864864864864864], [0, 0.4162162162162161], [0, 0.45675675675675664], [0, 0.7189189189189188], [0, 0.5675675675675677], [0, 0.281081081081081], [0, 0.5945945945945946], [0, 0.5378378378378377], [0, 0.05405405405405404], [0, 0.42702702702702705], [0, 0.6513513513513512], [0, 0.7999999999999999], [0, 0.8999999999999999], [0, 0.47567567567567565], [0, 0.5621621621621622], [0, 0.0], [0, 0.4810810810810811], [0, 0.47567567567567565], [0, 0.8324324324324324]]\n--------------------------------------------------------\n[[18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [17, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [13, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [17, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [11, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [11, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [15, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0], [12, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [13, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [17, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [12, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [13, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [12, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [14, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [19, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [13, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [17, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [19, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [12, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [9, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [17, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [14, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [17, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [12, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [19, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [20, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [20, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [15, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [17, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [13, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [13, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [12, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [19, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [20, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [19, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [19, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [13, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [13, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [14, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [14, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [17, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [13, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [19, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [14, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [10, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [15, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [12, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [16, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [20, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [16, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [20, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [16, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [15, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [15, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], [15, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [18, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0]]\n--------------------------------------------------------\n[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n[[0, 0.5459459459459457], [0, 0.2567567567567567], [0, 0.31621621621621604], [0, 0.4], [0, 0.45945945945945954], [0, 0.5729729729729728], [0, 0.7243243243243243], [0, 0.6648648648648647], [0, 0.8054054054054054], [0, 0.5000000000000001], [0, 0.2243243243243244], [0, 0.6081081081081082], [0, 0.0702702702702703], [0, 0.5567567567567567], [0, 0.6432432432432432], [0, 0.843243243243243], [0, 0.6864864864864864], [0, 0.48378378378378367], [0, 0.4729729729729731], [0, 0.15945945945945939], [0, 0.3783783783783785], [0, 0.6729729729729728], [0, 0.22162162162162166], [0, 0.6999999999999998], [0, 0.8135135135135134], [0, 0.2378378378378379], [0, 0.4351351351351351], [0, 0.7567567567567568], [0, 0.5648648648648648], [0, 0.22702702702702696], [0, 0.1864864864864864], [0, 0.5297297297297296], [0, 0.8513513513513514], [0, 0.35405405405405405], [0, 0.4108108108108106], [0, 0.5270270270270271], [0, 0.6297297297297297], [0, 0.5135135135135136], [0, 0.7621621621621619], [0, 0.4486486486486486], [0, 0.808108108108108], [0, 0.5162162162162162], [0, 0.5729729729729728], [0, 0.4810810810810811], [0, 0.327027027027027], [0, 0.5864864864864863], [0, 0.7297297297297298], [0, 0.8648648648648649], [0, 0.4729729729729731], [0, 0.943243243243243], [0, 0.5783783783783782], [0, 0.1540540540540541], [0, 0.29189189189189196], [0, 0.3972972972972971], [0, 0.4], [0, 0.4513513513513511], [0, 0.46486486486486467], [0, 0.418918918918919], [0, 0.5945945945945946], [0, 0.4540540540540541], [0, 0.6432432432432432], [0, 0.33513513513513504], [0, 0.5189189189189187], [0, 0.6459459459459458], [0, 0.5162162162162162], [0, 0.5243243243243242], [0, 0.6270270270270268], [0, 0.43783783783783764], [0, 0.4918918918918917], [0, 0.47837837837837816], [0, 0.7945945945945944], [0, 0.7459459459459459], [0, 0.924324324324324], [0, 0.29459459459459447], [0, 0.8675675675675675], [0, 0.5351351351351351], [0, 0.6189189189189188], [0, 0.2486486486486487], [0, 0.20270270270270266], [0, 0.7756756756756754], [0, 0.7567567567567568], [0, 0.34324324324324307], [0, 0.6378378378378378], [0, 0.6081081081081082], [0, 0.9027027027027025], [0, 0.28648648648648645], [0, 0.6027027027027027], [0, 0.1864864864864864], [0, 0.15675675675675685], [0, 0.6054054054054052], [0, 0.418918918918919], [0, 0.28378378378378394], [0, 0.2702702702702704], [0, 0.5405405405405407], [0, 0.5999999999999998], [0, 0.7621621621621619], [0, 0.12972972972972982], [0, 0.5027027027027027], [0, 0.6243243243243243], [0, 0.6675675675675673], [0, 0.5594594594594592], [0, 0.7432432432432433], [0, 0.5405405405405407], [0, 0.5567567567567567], [0, 0.02432432432432428], [0, 0.26486486486486494], [0, 0.1918918918918919], [0, 0.032432432432432504], [0, 0.8027027027027024], [0, 0.6945945945945944], [0, 0.7432432432432433], [0, 0.7756756756756754], [0, 0.42432432432432415], [0, 0.04324324324324327], [0, 0.47027027027027013], [0, 0.26486486486486494], [0, 0.327027027027027], [0, 0.3324324324324325], [0, 0.4540540540540541], [0, 0.7729729729729728], [0, 0.2297297297297297], [0, 0.7837837837837838], [0, 0.5432432432432431], [0, 0.935135135135135], [0, 0.6783783783783783], [0, 0.8243243243243243], [0, 0.040540540540540536], [0, 0.47027027027027013], [0, 0.5459459459459457], [0, 0.10810810810810809], [0, 0.6837837837837838], [0, 0.05675675675675679], [0, 0.7135135135135133], [0, 0.7783783783783783], [0, 0.09189189189189184], [0, 0.6864864864864864], [0, 0.4162162162162161], [0, 0.45675675675675664], [0, 0.7189189189189188], [0, 0.5675675675675677], [0, 0.281081081081081], [0, 0.5945945945945946], [0, 0.5378378378378377], [0, 0.05405405405405404], [0, 0.42702702702702705], [0, 0.6513513513513512], [0, 0.7999999999999999], [0, 0.8999999999999999], [0, 0.47567567567567565], [0, 0.5621621621621622], [0, 0.0], [0, 0.4810810810810811], [0, 0.47567567567567565], [0, 0.8324324324324324]]\n1 (166, 15)\n(166, 15)\n['RID', 'VISCODE', 'AGE', 'ADAS11', 'ADAS13', 'ADASQ4', 'MMSE', 'Ventricles', 'Hippocampus', 'WholeBrain', 'Entorhinal', 'Fusiform', 'MidTemp', 'ICV', 'DX']\nUla Ula Ula :  []\nagainnnnn\n15 12\n1.0769230769230769\n2\ninside: dataframe at least has one time point for data and ftp for prediction\n(166, 15) (107, 21) 166 107\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/2449308697.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mpkl_files_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There is an error! Please run it again.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data is ready as pkl files.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_28/494789487.py\u001b[0m in \u001b[0;36mpkl_files_creator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     X_train, y_train, demographic_train_data = create_dataset_train(train, number_of_training_visits,number_of_future_visits,\n\u001b[0;32m---> 96\u001b[0;31m                                                                     demographic_df)\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;31m# train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mlon_train_data_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_28/2129794371.py\u001b[0m in \u001b[0;36mcreate_dataset_train\u001b[0;34m(train_data_list, ts, fts, demographic_df)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcreate_train_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemographic_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#         print(create_train_lists(train_df_list[i], demographic_df, time_steps, fts))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_28/359107945.py\u001b[0m in \u001b[0;36mcreate_train_lists\u001b[0;34m(longitudinal_df, demographic_df, tp, ftp)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_feature_in_tp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mdemographic_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_demographic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mtarget_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inside create_train_lists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 raise ValueError(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1407\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"],"ename":"IndexError","evalue":"single positional indexer is out-of-bounds","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}