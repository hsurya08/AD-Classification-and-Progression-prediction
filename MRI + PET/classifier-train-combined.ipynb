{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import required lib","metadata":{}},{"cell_type":"code","source":"\npip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:26.387239Z","iopub.execute_input":"2023-04-23T00:23:26.387797Z","iopub.status.idle":"2023-04-23T00:23:40.256617Z","shell.execute_reply.started":"2023-04-23T00:23:26.387754Z","shell.execute_reply":"2023-04-23T00:23:40.254837Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install SimpleITK","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:40.259307Z","iopub.execute_input":"2023-04-23T00:23:40.260113Z","iopub.status.idle":"2023-04-23T00:23:52.108922Z","shell.execute_reply.started":"2023-04-23T00:23:40.260066Z","shell.execute_reply":"2023-04-23T00:23:52.107404Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: SimpleITK in /opt/conda/lib/python3.7/site-packages (2.2.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install nibabel","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:23:52.110979Z","iopub.execute_input":"2023-04-23T00:23:52.111395Z","iopub.status.idle":"2023-04-23T00:24:03.908003Z","shell.execute_reply.started":"2023-04-23T00:23:52.111353Z","shell.execute_reply":"2023-04-23T00:24:03.906291Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nibabel in /opt/conda/lib/python3.7/site-packages (4.0.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nibabel) (59.8.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from nibabel) (23.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from nibabel) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.transforms import RandomHorizontalFlip, RandomRotation, ToTensor, Normalize\nimport nibabel as nib\nfrom scipy.ndimage import zoom\nimport torch.nn as nn\nfrom torchvision.models.video import r3d_18\nimport pandas as pd\nimport SimpleITK as sitk\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T00:24:03.911449Z","iopub.execute_input":"2023-04-23T00:24:03.911865Z","iopub.status.idle":"2023-04-23T00:24:09.106067Z","shell.execute_reply.started":"2023-04-23T00:24:03.911823Z","shell.execute_reply":"2023-04-23T00:24:09.104926Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"final_df = pd.read_csv('/kaggle/input/output/final_output.csv')\nfinal_df.info(verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:24:09.107664Z","iopub.execute_input":"2023-04-23T00:24:09.108505Z","iopub.status.idle":"2023-04-23T00:24:09.175276Z","shell.execute_reply.started":"2023-04-23T00:24:09.108461Z","shell.execute_reply":"2023-04-23T00:24:09.173856Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2113 entries, 0 to 2112\nData columns (total 7 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   Subject        2113 non-null   object\n 1   Age            2113 non-null   int64 \n 2   Modality       2113 non-null   object\n 3   Group          2113 non-null   object\n 4   Image Data ID  2113 non-null   int64 \n 5   path           2113 non-null   object\n 6   filename       2113 non-null   object\ndtypes: int64(2), object(5)\nmemory usage: 115.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"mri_df = final_df[final_df['Modality'] == 'MRI']\nmri_paths = (mri_df['path'] +'/'+mri_df['filename']).to_numpy()\nmri_labels = mri_df['Group'].to_numpy()\n\npet_df = final_df[final_df['Modality'] == 'PET']\npet_paths = (pet_df['path'] +'/'+pet_df['filename']).to_numpy()\npet_labels = pet_df['Group'].to_numpy()\n\nmri_train_image_paths, mri_test_image_paths, mri_train_labels, mri_test_labels = train_test_split(\n    mri_paths, mri_labels, test_size=0.3, random_state=42)\n\n\n#PET\npet_train_image_paths, pet_test_image_paths, pet_train_labels, pet_test_labels = train_test_split(\n    pet_paths, pet_labels, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:24:09.177143Z","iopub.execute_input":"2023-04-23T00:24:09.178634Z","iopub.status.idle":"2023-04-23T00:24:09.196824Z","shell.execute_reply.started":"2023-04-23T00:24:09.178573Z","shell.execute_reply":"2023-04-23T00:24:09.195242Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Load the excel file with paths","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/pickle-files/mri_train_array.pkl', 'rb') as f:\n    mri_train_images = pickle.load(f)\n\nwith open('/kaggle/input/pickle-files/mri_test_array.pkl', 'rb') as f:\n    mri_test_images = pickle.load(f)\n    \nwith open('/kaggle/input/pickle-files/pet_train_array.pkl', 'rb') as f:\n    pet_train_images = pickle.load(f)\n\nwith open('/kaggle/input/pickle-files/pet_test_array.pkl', 'rb') as f:\n    pet_test_images = pickle.load(f)\n\nprint(mri_train_images.shape)\nprint(mri_test_images.shape)\n\nprint(pet_train_images.shape)\nprint(pet_test_images.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:24:09.198653Z","iopub.execute_input":"2023-04-23T00:24:09.199367Z","iopub.status.idle":"2023-04-23T00:25:46.271107Z","shell.execute_reply.started":"2023-04-23T00:24:09.199305Z","shell.execute_reply":"2023-04-23T00:25:46.268785Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(510, 60, 128, 128)\n(219, 60, 128, 128)\n(968, 60, 128, 128)\n(416, 60, 128, 128)\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_classes(train_labels):\n    unique_labels, counts = np.unique(train_labels, return_counts=True)\n    for label, count in zip(unique_labels, counts):\n        print(f\"Number of images in class {label}: {count}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:46.273954Z","iopub.execute_input":"2023-04-23T00:25:46.274490Z","iopub.status.idle":"2023-04-23T00:25:46.285190Z","shell.execute_reply.started":"2023-04-23T00:25:46.274436Z","shell.execute_reply":"2023-04-23T00:25:46.283458Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"mri_unique_labels, mri_counts = np.unique(mri_train_labels, return_counts=True)\nfor label, count in zip(mri_unique_labels, mri_counts):\n    print(f\"Number of images in class {label}: {count}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:46.287076Z","iopub.execute_input":"2023-04-23T00:25:46.287526Z","iopub.status.idle":"2023-04-23T00:25:46.302388Z","shell.execute_reply.started":"2023-04-23T00:25:46.287462Z","shell.execute_reply":"2023-04-23T00:25:46.300879Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Number of images in class AD: 67\nNumber of images in class CN: 149\nNumber of images in class MCI: 294\n","output_type":"stream"}]},{"cell_type":"code","source":"pet_unique_labels, pet_counts = np.unique(pet_train_labels, return_counts=True)\nfor label, count in zip(pet_unique_labels, pet_counts):\n    print(f\"Number of images in class {label}: {count}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:46.307515Z","iopub.execute_input":"2023-04-23T00:25:46.308386Z","iopub.status.idle":"2023-04-23T00:25:46.316828Z","shell.execute_reply.started":"2023-04-23T00:25:46.308329Z","shell.execute_reply":"2023-04-23T00:25:46.315341Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Number of images in class AD: 116\nNumber of images in class CN: 298\nNumber of images in class MCI: 554\n","output_type":"stream"}]},{"cell_type":"code","source":"# Number of images to extract for each label\nnum_images_per_label = {\"AD\": 67, \"CN\": 149, \"MCI\": 294}\n\n# Extract images for each label\nextracted_images = []\nextracted_labels = []\nfor label in num_images_per_label:\n    # Get indices of images with current label\n    label_indices = np.where(pet_train_labels == label)[0]\n    \n    # Select random subset of images with current label\n    selected_indices = np.random.choice(label_indices, size=num_images_per_label[label], replace=False)\n    selected_images = pet_train_images[selected_indices]\n    selected_labels = pet_train_labels[selected_indices]\n    \n    # Add selected images to extracted_images list\n    extracted_images.append(selected_images)\n    extracted_labels.append(selected_labels)\n    \n\n# Combine extracted images for all labels\npet_extracted_images = np.concatenate(extracted_images)\npet_extracted_labels = np.concatenate(extracted_labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:46.318821Z","iopub.execute_input":"2023-04-23T00:25:46.319686Z","iopub.status.idle":"2023-04-23T00:25:49.669235Z","shell.execute_reply.started":"2023-04-23T00:25:46.319627Z","shell.execute_reply":"2023-04-23T00:25:49.667714Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pet_unique_labels, pet_counts = np.unique(pet_extracted_labels, return_counts=True)\nfor label, count in zip(pet_unique_labels, pet_counts):\n    print(f\"Number of images in class {label}: {count}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:49.670857Z","iopub.execute_input":"2023-04-23T00:25:49.672686Z","iopub.status.idle":"2023-04-23T00:25:49.682971Z","shell.execute_reply.started":"2023-04-23T00:25:49.672628Z","shell.execute_reply":"2023-04-23T00:25:49.681345Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Number of images in class AD: 67\nNumber of images in class CN: 149\nNumber of images in class MCI: 294\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_labels, numerical_labels = np.unique(mri_train_labels, return_inverse=True)\nsorted_indices = np.argsort(numerical_labels)\nmri_sorted_images = mri_train_images[sorted_indices]\nmri_sorted_labels = mri_train_labels[sorted_indices]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:49.684708Z","iopub.execute_input":"2023-04-23T00:25:49.685076Z","iopub.status.idle":"2023-04-23T00:25:51.980034Z","shell.execute_reply.started":"2023-04-23T00:25:49.685040Z","shell.execute_reply":"2023-04-23T00:25:51.977622Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(mri_sorted_images.shape)\nprint(mri_sorted_labels.shape)\nprint(mri_sorted_labels)\nprint(pet_extracted_images.shape)\nprint(pet_extracted_labels.shape)\nprint(pet_extracted_labels)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:51.983396Z","iopub.execute_input":"2023-04-23T00:25:51.984163Z","iopub.status.idle":"2023-04-23T00:25:51.999474Z","shell.execute_reply.started":"2023-04-23T00:25:51.984078Z","shell.execute_reply":"2023-04-23T00:25:51.997469Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(510, 60, 128, 128)\n(510,)\n['AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI']\n(510, 60, 128, 128)\n(510,)\n['AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD'\n 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'AD' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'CN'\n 'CN' 'CN' 'CN' 'CN' 'CN' 'CN' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI']\n","output_type":"stream"}]},{"cell_type":"code","source":"classes = np.array(['MCI', 'CN', 'AD'])\n# Create a dictionary to map class names to their corresponding index values\nclass_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n# Use a list comprehension to convert the class names to index values\nmri_labels_idx = np.array([class_to_idx[label] for label in mri_sorted_labels])\n# Use the index values to create the one-hot encoded labels\nmri_lb_oh = np.eye(len(classes))[mri_labels_idx]\nmri_train_data_model = mri_sorted_images[:, np.newaxis, :,:,:]\n\n\npet_labels_idx = np.array([class_to_idx[label] for label in pet_extracted_labels])\npet_lb_oh = np.eye(len(classes))[pet_labels_idx]\npet_train_data_model = pet_extracted_images[:, np.newaxis, :,:,:]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:52.002332Z","iopub.execute_input":"2023-04-23T00:25:52.003548Z","iopub.status.idle":"2023-04-23T00:25:52.027069Z","shell.execute_reply.started":"2023-04-23T00:25:52.003464Z","shell.execute_reply":"2023-04-23T00:25:52.024333Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(mri_train_data_model.shape)\nprint(mri_lb_oh.shape)\nprint(pet_train_data_model.shape)\nprint(pet_lb_oh.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:52.029718Z","iopub.execute_input":"2023-04-23T00:25:52.031549Z","iopub.status.idle":"2023-04-23T00:25:52.055385Z","shell.execute_reply.started":"2023-04-23T00:25:52.031439Z","shell.execute_reply":"2023-04-23T00:25:52.053128Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(510, 1, 60, 128, 128)\n(510, 3)\n(510, 1, 60, 128, 128)\n(510, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RESNET model implementation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, ZeroPadding3D,\\\n     Flatten, BatchNormalization, AveragePooling3D, Dense, Activation, Add  , Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import activations\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras import optimizers","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:25:52.058006Z","iopub.execute_input":"2023-04-23T00:25:52.058820Z","iopub.status.idle":"2023-04-23T00:26:03.402389Z","shell.execute_reply.started":"2023-04-23T00:25:52.058737Z","shell.execute_reply":"2023-04-23T00:26:03.400927Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def res_identity(x, filters): \n  #renet block where dimension doesnot change.\n  #The skip connection is just simple identity conncection\n  #we will have 3 blocks and then input will be added\n\n  x_skip = x # this will be used for addition with the residual block \n  f1, f2 = filters\n\n  #first block \n  x = Conv3D(f1, kernel_size=(1,1, 1), strides=(1, 1,1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #second block # bottleneck (but size kept same with padding)\n  x = Conv3D(f1, kernel_size=(3,3, 3), strides=(1,1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # third block activation used after adding the input\n  x = Conv3D(f2, kernel_size=(1,1, 1), strides=(1,1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  # x = Activation(activations.relu)(x)\n\n  # add the input \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x\n\ndef res_conv(x, s, filters):\n  '''\n  here the input size changes''' \n  x_skip = x\n  f1, f2 = filters\n\n  # first block\n  x = Conv3D(f1, kernel_size=(1,1, 1), strides=(s,s, s), padding='valid', kernel_regularizer=l2(0.001))(x)\n  # when s = 2 then it is like downsizing the feature map\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  # second block\n  x = Conv3D(f1, kernel_size=(3,3, 3), strides=(1,1, 1), padding='same', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n\n  #third block\n  x = Conv3D(f2, kernel_size=(1,1, 1), strides=(1,1, 1), padding='valid', kernel_regularizer=l2(0.001))(x)\n  x = BatchNormalization()(x)\n\n  # shortcut \n  x_skip = Conv3D(f2, kernel_size=(1, 1,1), strides=(s,s, s), padding='valid', kernel_regularizer=l2(0.001))(x_skip)\n  x_skip = BatchNormalization()(x_skip)\n\n  # add \n  x = Add()([x, x_skip])\n  x = Activation(activations.relu)(x)\n\n  return x","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:03.404221Z","iopub.execute_input":"2023-04-23T00:26:03.405129Z","iopub.status.idle":"2023-04-23T00:26:03.423596Z","shell.execute_reply.started":"2023-04-23T00:26:03.405084Z","shell.execute_reply":"2023-04-23T00:26:03.422093Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def resnet50mri():\n\n  input_im = Input(shape=(1,60, 128, 128,)) # cifar 10 images size,\n  x = ZeroPadding3D(padding=(1,1, 1))(input_im)\n\n  # 1st stage\n  # here we perform maxpooling, see the figure above\n\n  x = Conv3D(64, kernel_size=(3, 3,3), strides=(1,1, 1))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n  x = MaxPooling3D((1,1, 1), strides=(1, 1,1))(x)\n\n  #2nd stage \n  # frm here on only conv block and identity block, no pooling\n\n  x = res_conv(x, s=1, filters=(16, 64))\n  x = res_identity(x, filters=(16, 64))\n  x = res_identity(x, filters=(16, 64))\n\n  # 3rd stage\n\n  x = res_conv(x, s=2, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # 4th stage\n\n  x = res_conv(x, s=2, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # 5th stage\n\n  x = res_conv(x, s=2, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # ends with average pooling and dense connection\n\n  x = AveragePooling3D((2,2, 2), padding='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(3, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n\n  # define the model \n\n  model = Model(inputs=input_im, outputs=x, name='Resnet50')\n\n  return model","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:03.424953Z","iopub.execute_input":"2023-04-23T00:26:03.426223Z","iopub.status.idle":"2023-04-23T00:26:03.441999Z","shell.execute_reply.started":"2023-04-23T00:26:03.426163Z","shell.execute_reply":"2023-04-23T00:26:03.440313Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def resnet50pet():\n\n  input_im = Input(shape=(1,60, 128, 128,)) # cifar 10 images size,\n  x = ZeroPadding3D(padding=(1,1, 1))(input_im)\n\n  # 1st stage\n  # here we perform maxpooling, see the figure above\n\n  x = Conv3D(64, kernel_size=(3, 3,3), strides=(1,1, 1))(x)\n  x = BatchNormalization()(x)\n  x = Activation(activations.relu)(x)\n  x = MaxPooling3D((1,1, 1), strides=(1, 1,1))(x)\n\n  #2nd stage \n  # frm here on only conv block and identity block, no pooling\n\n  x = res_conv(x, s=1, filters=(16, 64))\n  x = res_identity(x, filters=(16, 64))\n  x = res_identity(x, filters=(16, 64))\n\n  # 3rd stage\n\n  x = res_conv(x, s=2, filters=(64, 128))\n  x = res_identity(x, filters=(64, 128))\n  x = res_identity(x, filters=(64, 128))\n  x = res_identity(x, filters=(64, 128))\n\n  # 4th stage\n\n  x = res_conv(x, s=2, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # 5th stage\n\n  x = res_conv(x, s=2, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n  x = res_identity(x, filters=(64, 256))\n\n  # ends with average pooling and dense connection\n\n  x = AveragePooling3D((2,2, 2), padding='same')(x)\n\n  x = Flatten()(x)\n  x = Dense(3, activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n\n  # define the model \n\n  model = Model(inputs=input_im, outputs=x, name='Resnet50')\n\n  return model","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:03.443703Z","iopub.execute_input":"2023-04-23T00:26:03.444160Z","iopub.status.idle":"2023-04-23T00:26:03.464477Z","shell.execute_reply.started":"2023-04-23T00:26:03.444116Z","shell.execute_reply":"2023-04-23T00:26:03.462782Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"base_model_MRI =resnet50mri()\n\nfc_layer = tf.keras.layers.Dense(512, activation='relu')(base_model_MRI.layers[-2].output)\nfc_layer = tf.keras.layers.Dropout(0.8)(fc_layer)\nfc_layer = Dense(16)(fc_layer)\nfc_layer = Model(inputs=base_model_MRI.input, outputs=fc_layer)\n\n\nbase_model_PET = resnet50pet()\n\nfc_layer1 = tf.keras.layers.Dense(512, activation='relu')(base_model_PET.layers[-2].output)\nfc_layer1 = tf.keras.layers.Dropout(0.8)(fc_layer1)\nfc_layer1 = Dense(16)(fc_layer1)\nfc_layer1 = Model(inputs=base_model_PET.input, outputs=fc_layer1)\n\nz = Concatenate()([fc_layer.output, fc_layer1.output])\n# z = Dense(16, activation=\"relu\")(combined)\noutput_layer = tf.keras.layers.Dense(3, activation='sigmoid')(z)\nmodel = tf.keras.models.Model(inputs=[base_model_MRI.input,base_model_PET.input], outputs=output_layer)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:03.466075Z","iopub.execute_input":"2023-04-23T00:26:03.466450Z","iopub.status.idle":"2023-04-23T00:26:07.724665Z","shell.execute_reply.started":"2023-04-23T00:26:03.466415Z","shell.execute_reply":"2023-04-23T00:26:07.723145Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:07.726156Z","iopub.execute_input":"2023-04-23T00:26:07.726583Z","iopub.status.idle":"2023-04-23T00:26:07.732598Z","shell.execute_reply.started":"2023-04-23T00:26:07.726543Z","shell.execute_reply":"2023-04-23T00:26:07.730845Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"mri_data = mri_train_data_model\npet_data = pet_train_data_model\n\nlabels = mri_lb_oh","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:07.734087Z","iopub.execute_input":"2023-04-23T00:26:07.734465Z","iopub.status.idle":"2023-04-23T00:26:07.746072Z","shell.execute_reply.started":"2023-04-23T00:26:07.734427Z","shell.execute_reply":"2023-04-23T00:26:07.744848Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(mri_train_data_model.shape)\nprint(pet_train_data_model.shape)\nprint(labels[:15])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:07.747621Z","iopub.execute_input":"2023-04-23T00:26:07.748772Z","iopub.status.idle":"2023-04-23T00:26:07.763126Z","shell.execute_reply.started":"2023-04-23T00:26:07.748726Z","shell.execute_reply":"2023-04-23T00:26:07.761546Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(510, 1, 60, 128, 128)\n(510, 1, 60, 128, 128)\n[[0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]\n [0. 0. 1.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile('adam','categorical_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:07.764693Z","iopub.execute_input":"2023-04-23T00:26:07.765875Z","iopub.status.idle":"2023-04-23T00:26:07.804976Z","shell.execute_reply.started":"2023-04-23T00:26:07.765817Z","shell.execute_reply":"2023-04-23T00:26:07.803156Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"num_samples = len(labels)\nshuffle_indices = np.random.permutation(num_samples)\n\n# shuffle images and labels arrays using the shuffled indices\nshuffled_mri_data = mri_data[shuffle_indices]\nshuffled_pet_data = pet_data[shuffle_indices]\nshuffled_labels = labels[shuffle_indices]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:07.806935Z","iopub.execute_input":"2023-04-23T00:26:07.807357Z","iopub.status.idle":"2023-04-23T00:26:11.496007Z","shell.execute_reply.started":"2023-04-23T00:26:07.807319Z","shell.execute_reply":"2023-04-23T00:26:11.494658Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print(shuffled_mri_data.shape)\nprint(shuffled_pet_data.shape)\nprint(shuffled_labels[:15])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:11.497892Z","iopub.execute_input":"2023-04-23T00:26:11.498291Z","iopub.status.idle":"2023-04-23T00:26:11.506636Z","shell.execute_reply.started":"2023-04-23T00:26:11.498240Z","shell.execute_reply":"2023-04-23T00:26:11.505303Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"(510, 1, 60, 128, 128)\n(510, 1, 60, 128, 128)\n[[0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit([shuffled_mri_data,shuffled_pet_data],shuffled_labels,batch_size=16,epochs=50,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T00:26:11.512881Z","iopub.execute_input":"2023-04-23T00:26:11.513780Z","iopub.status.idle":"2023-04-23T05:31:38.335328Z","shell.execute_reply.started":"2023-04-23T00:26:11.513719Z","shell.execute_reply":"2023-04-23T05:31:38.331941Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1/50\n32/32 [==============================] - 432s 12s/step - loss: 22.5321 - accuracy: 0.4157\nEpoch 2/50\n32/32 [==============================] - 383s 12s/step - loss: 12.6106 - accuracy: 0.5000\nEpoch 3/50\n32/32 [==============================] - 366s 11s/step - loss: 10.8142 - accuracy: 0.4412\nEpoch 4/50\n32/32 [==============================] - 365s 11s/step - loss: 9.8204 - accuracy: 0.5137\nEpoch 5/50\n32/32 [==============================] - 374s 12s/step - loss: 8.7415 - accuracy: 0.5529\nEpoch 6/50\n32/32 [==============================] - 371s 12s/step - loss: 8.2657 - accuracy: 0.5490\nEpoch 7/50\n32/32 [==============================] - 368s 11s/step - loss: 8.0916 - accuracy: 0.5804\nEpoch 8/50\n32/32 [==============================] - 363s 11s/step - loss: 7.8864 - accuracy: 0.5706\nEpoch 9/50\n32/32 [==============================] - 367s 11s/step - loss: 7.7690 - accuracy: 0.5902\nEpoch 10/50\n32/32 [==============================] - 370s 12s/step - loss: 7.7123 - accuracy: 0.6157\nEpoch 11/50\n32/32 [==============================] - 366s 11s/step - loss: 7.5495 - accuracy: 0.6176\nEpoch 12/50\n32/32 [==============================] - 367s 11s/step - loss: 7.4300 - accuracy: 0.6157\nEpoch 13/50\n32/32 [==============================] - 363s 11s/step - loss: 7.3416 - accuracy: 0.6157\nEpoch 14/50\n32/32 [==============================] - 362s 11s/step - loss: 7.2089 - accuracy: 0.6137\nEpoch 15/50\n32/32 [==============================] - 366s 11s/step - loss: 7.1249 - accuracy: 0.6255\nEpoch 16/50\n32/32 [==============================] - 365s 11s/step - loss: 6.9864 - accuracy: 0.6314\nEpoch 17/50\n32/32 [==============================] - 365s 11s/step - loss: 6.9061 - accuracy: 0.6490\nEpoch 18/50\n32/32 [==============================] - 361s 11s/step - loss: 6.7830 - accuracy: 0.6392\nEpoch 19/50\n32/32 [==============================] - 363s 11s/step - loss: 6.6977 - accuracy: 0.6294\nEpoch 20/50\n32/32 [==============================] - 365s 11s/step - loss: 6.5307 - accuracy: 0.6706\nEpoch 21/50\n32/32 [==============================] - 362s 11s/step - loss: 6.4427 - accuracy: 0.6804\nEpoch 22/50\n32/32 [==============================] - 364s 11s/step - loss: 6.3257 - accuracy: 0.6980\nEpoch 23/50\n32/32 [==============================] - 364s 11s/step - loss: 6.1593 - accuracy: 0.7490\nEpoch 24/50\n32/32 [==============================] - 361s 11s/step - loss: 6.0653 - accuracy: 0.7333\nEpoch 25/50\n32/32 [==============================] - 368s 11s/step - loss: 6.0175 - accuracy: 0.7078\nEpoch 26/50\n32/32 [==============================] - 371s 12s/step - loss: 5.8282 - accuracy: 0.7412\nEpoch 27/50\n32/32 [==============================] - 383s 12s/step - loss: 5.7006 - accuracy: 0.7706\nEpoch 28/50\n32/32 [==============================] - 363s 11s/step - loss: 5.5757 - accuracy: 0.7843\nEpoch 29/50\n32/32 [==============================] - 364s 11s/step - loss: 5.4961 - accuracy: 0.7725\nEpoch 30/50\n32/32 [==============================] - 366s 11s/step - loss: 5.4050 - accuracy: 0.7765\nEpoch 31/50\n32/32 [==============================] - 362s 11s/step - loss: 5.2400 - accuracy: 0.7961\nEpoch 32/50\n32/32 [==============================] - 363s 11s/step - loss: 5.1460 - accuracy: 0.8216\nEpoch 33/50\n32/32 [==============================] - 360s 11s/step - loss: 5.0273 - accuracy: 0.8431\nEpoch 34/50\n32/32 [==============================] - 359s 11s/step - loss: 4.8516 - accuracy: 0.8549\nEpoch 35/50\n32/32 [==============================] - 362s 11s/step - loss: 4.6893 - accuracy: 0.8902\nEpoch 36/50\n32/32 [==============================] - 362s 11s/step - loss: 4.7198 - accuracy: 0.8686\nEpoch 37/50\n32/32 [==============================] - 362s 11s/step - loss: 4.6544 - accuracy: 0.8490\nEpoch 38/50\n32/32 [==============================] - 367s 11s/step - loss: 4.5903 - accuracy: 0.8569\nEpoch 39/50\n32/32 [==============================] - 364s 11s/step - loss: 4.6152 - accuracy: 0.8314\nEpoch 40/50\n32/32 [==============================] - 362s 11s/step - loss: 4.5076 - accuracy: 0.8196\nEpoch 41/50\n32/32 [==============================] - 363s 11s/step - loss: 4.3900 - accuracy: 0.8412\nEpoch 42/50\n32/32 [==============================] - 364s 11s/step - loss: 4.2845 - accuracy: 0.8725\nEpoch 43/50\n32/32 [==============================] - 364s 11s/step - loss: 4.1548 - accuracy: 0.8706\nEpoch 44/50\n32/32 [==============================] - 363s 11s/step - loss: 4.0948 - accuracy: 0.8588\nEpoch 45/50\n32/32 [==============================] - 362s 11s/step - loss: 4.0327 - accuracy: 0.8765\nEpoch 46/50\n32/32 [==============================] - 364s 11s/step - loss: 3.9168 - accuracy: 0.8902\nEpoch 47/50\n32/32 [==============================] - 361s 11s/step - loss: 3.8918 - accuracy: 0.8804\nEpoch 48/50\n32/32 [==============================] - 361s 11s/step - loss: 3.8975 - accuracy: 0.8294\nEpoch 49/50\n32/32 [==============================] - 362s 11s/step - loss: 3.9204 - accuracy: 0.8314\nEpoch 50/50\n32/32 [==============================] - 364s 11s/step - loss: 3.6378 - accuracy: 0.9235\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7ec45cae0610>"},"metadata":{}}]},{"cell_type":"code","source":"from keras.models import load_model\nmodel.save('mri_pet.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-23T05:33:03.690009Z","iopub.execute_input":"2023-04-23T05:33:03.690493Z","iopub.status.idle":"2023-04-23T05:33:05.331962Z","shell.execute_reply.started":"2023-04-23T05:33:03.690448Z","shell.execute_reply":"2023-04-23T05:33:05.330184Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"mri_test_images = np.array(mri_test_images)\nprint(mri_test_images.shape)\nprint(mri_test_labels)\n\n# Use a list comprehension to convert the class names to index values\nmri_test_labels_idx = np.array([class_to_idx[label] for label in mri_test_labels])\n\n# Use the index values to create the one-hot encoded labels\none_hot_test_labels = np.eye(len(classes))[mri_test_labels_idx]\nmri_test_data_model = mri_test_images[:, np.newaxis, :,:,:]\nprint(mri_test_data_model.shape)\nprint(mri_test_labels_idx)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T05:31:40.062909Z","iopub.execute_input":"2023-04-23T05:31:40.063387Z","iopub.status.idle":"2023-04-23T05:31:44.935637Z","shell.execute_reply.started":"2023-04-23T05:31:40.063342Z","shell.execute_reply":"2023-04-23T05:31:44.934216Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"(219, 60, 128, 128)\n['MCI' 'MCI' 'CN' 'MCI' 'AD' 'MCI' 'CN' 'AD' 'CN' 'MCI' 'MCI' 'AD' 'CN'\n 'CN' 'CN' 'AD' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'CN' 'CN' 'AD' 'CN' 'MCI'\n 'CN' 'MCI' 'AD' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'CN' 'MCI' 'CN' 'CN'\n 'CN' 'MCI' 'MCI' 'CN' 'MCI' 'CN' 'CN' 'MCI' 'CN' 'AD' 'MCI' 'CN' 'CN'\n 'AD' 'AD' 'MCI' 'MCI' 'CN' 'MCI' 'MCI' 'CN' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'CN' 'MCI' 'MCI' 'CN' 'CN' 'MCI' 'MCI' 'MCI' 'MCI'\n 'MCI' 'MCI' 'MCI' 'MCI' 'CN' 'MCI' 'AD' 'MCI' 'CN' 'MCI' 'MCI' 'MCI'\n 'MCI' 'AD' 'CN' 'CN' 'MCI' 'AD' 'MCI' 'MCI' 'AD' 'MCI' 'AD' 'CN' 'MCI'\n 'CN' 'CN' 'MCI' 'MCI' 'MCI' 'CN' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'MCI' 'AD'\n 'MCI' 'MCI' 'MCI' 'CN' 'CN' 'MCI' 'CN' 'MCI' 'MCI' 'MCI' 'CN' 'MCI' 'MCI'\n 'MCI' 'AD' 'AD' 'AD' 'AD' 'MCI' 'MCI' 'MCI' 'CN' 'CN' 'CN' 'MCI' 'CN'\n 'MCI' 'MCI' 'MCI' 'AD' 'MCI' 'CN' 'MCI' 'MCI' 'MCI' 'MCI' 'AD' 'CN' 'MCI'\n 'CN' 'CN' 'MCI' 'MCI' 'CN' 'CN' 'MCI' 'MCI' 'MCI' 'CN' 'MCI' 'MCI' 'CN'\n 'MCI' 'MCI' 'AD' 'MCI' 'CN' 'CN' 'MCI' 'MCI' 'CN' 'CN' 'AD' 'MCI' 'MCI'\n 'MCI' 'CN' 'CN' 'MCI' 'AD' 'AD' 'MCI' 'CN' 'MCI' 'MCI' 'CN' 'MCI' 'AD'\n 'MCI' 'AD' 'CN' 'MCI' 'MCI' 'CN' 'CN' 'MCI' 'CN' 'MCI' 'MCI' 'AD' 'MCI'\n 'MCI' 'AD' 'CN' 'CN' 'MCI' 'AD' 'CN' 'MCI' 'CN' 'MCI' 'MCI' 'MCI' 'MCI']\n(219, 1, 60, 128, 128)\n[0 0 1 0 2 0 1 2 1 0 0 2 1 1 1 2 0 0 0 0 0 1 1 2 1 0 1 0 2 0 0 0 0 0 0 1 0\n 1 1 1 0 0 1 0 1 1 0 1 2 0 1 1 2 2 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0\n 0 0 0 0 0 0 0 1 0 2 0 1 0 0 0 0 2 1 1 0 2 0 0 2 0 2 1 0 1 1 0 0 0 1 0 0 0\n 0 0 0 2 0 0 0 1 1 0 1 0 0 0 1 0 0 0 2 2 2 2 0 0 0 1 1 1 0 1 0 0 0 2 0 1 0\n 0 0 0 2 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 2 0 1 1 0 0 1 1 2 0 0 0 1 1 0 2\n 2 0 1 0 0 1 0 2 0 2 1 0 0 1 1 0 1 0 0 2 0 0 2 1 1 0 2 1 0 1 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred_probs = model.predict([mri_test_data_model,mri_test_data_model])  # Predicted probabilities for each class\ny_pred = np.argmax(y_pred_probs, axis=1)  # Convert probabilities to class labels\n\n# Compute the confusion matrix\nconfusion_mat = confusion_matrix(mri_test_labels_idx, y_pred)\n\n# Compute evaluation metrics\naccuracy = accuracy_score(mri_test_labels_idx, y_pred)\nprecision = precision_score(mri_test_labels_idx, y_pred, average='weighted')\nrecall = recall_score(mri_test_labels_idx, y_pred, average='weighted')\nf1score = f1_score(mri_test_labels_idx, y_pred, average='weighted')\n\nprint(\"Confusion Matrix:\\n\", confusion_mat)\nprint(\"Accuracy: {:.4f}\".format(accuracy))\nprint(\"Precision: {:.4f}\".format(precision))\nprint(\"Recall: {:.4f}\".format(recall))\nprint(\"F1-score: {:.4f}\".format(f1score))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T05:31:44.937127Z","iopub.execute_input":"2023-04-23T05:31:44.937527Z","iopub.status.idle":"2023-04-23T05:32:34.836795Z","shell.execute_reply.started":"2023-04-23T05:31:44.937488Z","shell.execute_reply":"2023-04-23T05:32:34.835737Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"7/7 [==============================] - 47s 6s/step\nConfusion Matrix:\n [[118   5   1]\n [  6  56   3]\n [ 10   0  20]]\nAccuracy: 0.8858\nPrecision: 0.8852\nRecall: 0.8858\nF1-score: 0.8832\n","output_type":"stream"}]},{"cell_type":"code","source":"print(mri_test_labels_idx)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T05:32:34.838418Z","iopub.execute_input":"2023-04-23T05:32:34.839136Z","iopub.status.idle":"2023-04-23T05:32:34.847311Z","shell.execute_reply.started":"2023-04-23T05:32:34.839083Z","shell.execute_reply":"2023-04-23T05:32:34.845837Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"[0 0 1 0 2 0 1 2 1 0 0 2 1 1 1 2 0 0 0 0 0 1 1 2 1 0 1 0 2 0 0 0 0 0 0 1 0\n 1 1 1 0 0 1 0 1 1 0 1 2 0 1 1 2 2 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0\n 0 0 0 0 0 0 0 1 0 2 0 1 0 0 0 0 2 1 1 0 2 0 0 2 0 2 1 0 1 1 0 0 0 1 0 0 0\n 0 0 0 2 0 0 0 1 1 0 1 0 0 0 1 0 0 0 2 2 2 2 0 0 0 1 1 1 0 1 0 0 0 2 0 1 0\n 0 0 0 2 1 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 2 0 1 1 0 0 1 1 2 0 0 0 1 1 0 2\n 2 0 1 0 0 1 0 2 0 2 1 0 0 1 1 0 1 0 0 2 0 0 2 1 1 0 2 1 0 1 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"#print(test_data_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}